{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import process\n",
    "text = 'data/brown.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import casual\n",
    "\n",
    "import json\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from process import load_model, load_msr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowList = [2,5,10]\n",
    "dimList = [100,300,1000]\n",
    "EPOCHS = 5\n",
    "iwin = 2\n",
    "idim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, 'r'):\n",
    "            yield [word.lower() for word in tokenizer.tokenize(line) if word.lower() not in stop_words]\n",
    "sentence = MySentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for x in sentence:\n",
    "    word_counts.update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42281\n",
      "[('one', 3504), ('would', 2719), ('said', 1961), ('time', 1695), ('new', 1646)]\n",
      "[('perelman', 1), ('exhaling', 1), ('aviary', 1), ('boucle', 1), ('stupefying', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counts))\n",
    "print(word_counts.most_common(5))\n",
    "print(word_counts.most_common()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13891\n",
      "[('one', 3504), ('would', 2719), ('said', 1961), ('time', 1695), ('new', 1646)]\n",
      "[('furrow', 5), ('richert', 5), ('kafka', 5), ('poitrine', 5), ('quasimodo', 5)]\n"
     ]
    }
   ],
   "source": [
    "# minimum word count = 5\n",
    "word_counts2 = Counter()\n",
    "for key,value in word_counts.items():\n",
    "    if value >= 5:\n",
    "        word_counts2[key] = value\n",
    "print(len(word_counts2))\n",
    "print(word_counts2.most_common(5))\n",
    "print(word_counts2.most_common()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497145\n",
      "[(('united', 'states'), 401), (('states', 'united'), 401), (('af', 'af'), 392), (('new', 'york'), 312), (('york', 'new'), 312)]\n"
     ]
    }
   ],
   "source": [
    "# count co-occurance\n",
    "joint = Counter()\n",
    "for isen in sentence:\n",
    "    for j, word in enumerate(isen):\n",
    "        if word not in word_counts:\n",
    "            continue\n",
    "        index_min = max(0, j-iwin)\n",
    "        index_max = min(len(isen), j+iwin+1)\n",
    "        index = [ii for ii in range(index_min, index_max) if ii!=j]\n",
    "        for iin in index:\n",
    "            joint[(word, isen[iin])] += 1\n",
    "print(len(joint))\n",
    "print(joint.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts2['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate index for words in vocabulary\n",
    "index_dict = dict()\n",
    "word_dict = dict()\n",
    "ii = 0\n",
    "for i in word_counts:\n",
    "    index_dict[i] = ii\n",
    "    word_dict[ii] = i\n",
    "    ii += 1\n",
    "    \n",
    "# transform count joint to sparse matrix\n",
    "import math\n",
    "row_index = []\n",
    "col_index = []\n",
    "values = []\n",
    "for (wi,wj), count in joint.items():\n",
    "    row = index_dict[wi]\n",
    "    col = index_dict[wj]\n",
    "    prod = word_counts[wi]*word_counts[wj]*len(joint)/(len(word_counts)**2)\n",
    "    if count > prod:\n",
    "        value = math.log(count/prod)\n",
    "    else:\n",
    "        value = 0\n",
    "    row_index.append(row)\n",
    "    col_index.append(col)\n",
    "    values.append(value)\n",
    "\n",
    "from scipy import sparse\n",
    "ppmi = sparse.csr_matrix((values, (row_index, col_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     0, ..., 42279, 42280, 42280], dtype=int32),\n",
       " array([    1,     2,     3, ..., 42280,  6523, 42279], dtype=int32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def ww_sim(word, mat, topn=10):\n",
    "    indx = index_dict[word]\n",
    "    if isinstance(mat, sparse.csr_matrix):\n",
    "        v1 = mat.getrow(indx)\n",
    "    else:\n",
    "        v1 = mat[indx:indx+1, :]\n",
    "    sims = cosine_similarity(mat, v1).flatten()\n",
    "    sindxs = np.argsort(-sims)\n",
    "#     print(sindxs[0:topn])\n",
    "#     print(index_dict[9254])\n",
    "#     print(sims[9254])\n",
    "    sim_word_scores = [(word_dict[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n",
    "    return sim_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uncle', 1.0),\n",
       " ('replanted', 0.19015442139538086),\n",
       " ('ffortescue', 0.18770341268912027),\n",
       " ('winnings', 0.1635886263178493),\n",
       " ('lorde', 0.14375627841760283),\n",
       " ('countrey', 0.14093206890510743),\n",
       " ('tussle', 0.13681876061169834),\n",
       " ('donald', 0.12825493146537434),\n",
       " ('underlay', 0.12695794690543205),\n",
       " ('stowe', 0.12417572821154328)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim('uncle', ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42281, 100) (100,) (100, 42281)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import linalg\n",
    "uu,ss,vv = linalg.svds(ppmi, idim)\n",
    "print(np.shape(uu),np.shape(ss),np.shape(vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/Users/mac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "unorm = uu / np.sqrt(np.sum(uu*uu, axis=1, keepdims=True))\n",
    "vnorm = vv / np.sqrt(np.sum(vv*vv, axis=0, keepdims=True))\n",
    "#word_vecs = unorm\n",
    "#word_vecs = vnorm.T\n",
    "word_vecs = uu + vv.T\n",
    "word_vecs_norm = word_vecs / np.sqrt(np.sum(word_vecs*word_vecs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 1.0000000000000004),\n",
       " ('choosy', 0.9837187516554718),\n",
       " ('parasite', 0.9767523687490764),\n",
       " ('matriarchal', 0.9579497486448949),\n",
       " ('amazons', 0.8838188316210213),\n",
       " ('dissenters', 0.8662304672042447),\n",
       " ('criticizing', 0.8620292444505535),\n",
       " ('colombian', 0.8430180909941202),\n",
       " ('merciless', 0.8284281431464853),\n",
       " ('factions', 0.8258929852339392)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim(\"female\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strike', 0.9999999999999998),\n",
       " ('soldiers', 0.804110429821317),\n",
       " ('unenthusiastic', 0.7801367386647553),\n",
       " ('gesturing', 0.7547880528517577),\n",
       " ('upraised', 0.7517724899072793),\n",
       " ('pressed', 0.7488827586046398),\n",
       " ('spruced', 0.7431111912300254),\n",
       " ('drifted', 0.7415945161082629),\n",
       " ('warmish', 0.7354715342614404),\n",
       " ('foul', 0.7351051738108774)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim(\"strike\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uncle', 1.0000000000000002),\n",
       " ('indisposition', 0.9978157634359457),\n",
       " ('doin', 0.9697134515392729),\n",
       " ('countrey', 0.9446329677743157),\n",
       " ('vinnicum', 0.943740872841573),\n",
       " ('conspires', 0.939735520007934),\n",
       " ('indisposed', 0.9332111812406453),\n",
       " ('ffortescue', 0.9292737667590284),\n",
       " ('exchequer', 0.9240052795821159),\n",
       " ('hush', 0.9179795742322708)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim(\"uncle\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sentences iterable can be simply a list, but for larger corpora, consider a generator that streams the sentences directly from disk/network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t0.0\n",
      "  (0, 7)\t0.0\n",
      "  (0, 29)\t0.0\n",
      "  (0, 30)\t0.0\n",
      "  (0, 31)\t0.0\n",
      "  (0, 58)\t0.0\n",
      "  (0, 59)\t0.0\n",
      "  (0, 60)\t0.0\n",
      "  (0, 64)\t0.0\n",
      "  (0, 66)\t0.0\n",
      "  (0, 67)\t0.0\n",
      "  (0, 90)\t0.0\n",
      "  (0, 106)\t0.0\n",
      "  (0, 119)\t0.0\n",
      "  (0, 129)\t0.0\n",
      "  (0, 130)\t0.0\n",
      "  (0, 131)\t0.0\n",
      "  (0, 138)\t0.0\n",
      "  (0, 139)\t0.0\n",
      "  (0, 149)\t0.0\n",
      "  (0, 150)\t0.0\n",
      "  (0, 151)\t0.0\n",
      "  (0, 156)\t0.0\n",
      "  (0, 157)\t0.0\n",
      "  (0, 158)\t0.0\n",
      "  (0, 210)\t0.0\n",
      "  (0, 211)\t0.0\n",
      "  (0, 215)\t0.0\n",
      "  (0, 222)\t0.0\n",
      "  (0, 235)\t0.0\n",
      "  (0, 236)\t0.0\n",
      "  (0, 237)\t0.0\n",
      "  (0, 241)\t0.6394438532335271\n",
      "  (0, 242)\t0.0\n",
      "  (0, 243)\t0.0\n",
      "  (0, 244)\t0.0\n",
      "  (0, 246)\t0.6394438532335271\n",
      "  (0, 252)\t0.0\n",
      "  (0, 304)\t0.0\n",
      "  (0, 1051)\t0.0\n",
      "  (0, 1212)\t0.0\n",
      "  (0, 2258)\t0.0\n",
      "  (0, 4037)\t0.0\n",
      "  (0, 4165)\t0.0\n",
      "  (0, 6212)\t0.0\n",
      "  (0, 6789)\t0.0\n"
     ]
    }
   ],
   "source": [
    "print(ppmi[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-d30a2fe9d97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../NLPdata/hw3/savedModel/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "word_vecs.save(\"../../NLPdata/hw3/savedModel/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13891, 100)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.63849412e-04, -9.70924308e-06, -3.89196601e-04, -1.99606045e-04,\n",
       "       -1.52572737e-04,  3.92952168e-04,  4.11227720e-16,  6.34080014e-04,\n",
       "       -8.64653808e-05, -1.56360244e-03, -5.49293412e-15, -2.56142763e-17,\n",
       "        1.16559588e-04, -4.07202049e-05,  4.32493179e-05,  2.21068619e-04,\n",
       "        3.71345924e-04,  4.73482216e-04,  8.14642407e-17, -4.92336396e-04,\n",
       "       -1.26622883e-04,  1.42008801e-15,  6.22602646e-04,  9.23833035e-05,\n",
       "       -1.18927200e-04,  9.85675300e-17,  2.16759825e-05, -4.91414635e-16,\n",
       "       -3.35825277e-04, -3.18863859e-16, -1.83852395e-04, -8.15787470e-05,\n",
       "        2.11504127e-17, -8.56977114e-17, -6.59427894e-05,  2.85409917e-04,\n",
       "       -1.09294355e-16,  6.32557204e-05,  5.77771367e-05,  8.97643609e-05,\n",
       "       -4.61098228e-05,  2.48408268e-05, -3.19926818e-04, -9.76574519e-05,\n",
       "       -5.61873316e-05,  6.93308960e-04,  3.10089953e-15, -1.51924772e-04,\n",
       "       -2.84483907e-05,  1.10950504e-04, -5.57999895e-17,  1.62489884e-05,\n",
       "       -1.04200530e-04, -5.91413011e-05,  3.16817047e-04,  5.76590544e-05,\n",
       "        1.64330426e-06, -5.69516610e-04,  9.03169577e-06, -1.88214933e-05,\n",
       "       -1.12542091e-04, -1.75870998e-04, -9.78829467e-05,  7.43822831e-05,\n",
       "        2.59406076e-04, -3.80664763e-04,  1.97051600e-04, -3.02960186e-05,\n",
       "        2.94307443e-05, -1.90648587e-04, -5.88568542e-05, -1.07930207e-04,\n",
       "       -4.16241997e-04,  4.30507776e-05,  1.45312311e-04,  1.95227415e-04,\n",
       "        4.36293493e-03, -4.09497445e-05, -1.27053849e-05, -1.73563958e-04,\n",
       "       -2.35633146e-04, -8.45838896e-05,  3.96022024e-05,  4.14037738e-05,\n",
       "        1.62074476e-05, -7.53819548e-06,  6.75414849e-05,  1.29570872e-06,\n",
       "       -3.44006714e-05,  5.28233248e-05, -2.69400607e-05,  2.61495000e-04,\n",
       "        8.93578253e-06,  6.34366818e-06, -6.09949907e-06,  7.08059894e-05,\n",
       "        3.08302097e-05, -9.56924279e-07,  3.64591979e-07,  2.60713917e-06])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
