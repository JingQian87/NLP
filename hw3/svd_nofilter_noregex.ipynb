{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import process\n",
    "text = 'data/brown.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import casual\n",
    "\n",
    "import json\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from process import load_model, load_msr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowList = [2,5,10]\n",
    "dimList = [100,300,1000]\n",
    "EPOCHS = 5\n",
    "iwin = 2\n",
    "idim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, 'r'):\n",
    "            yield [word.lower() for word in line.split()]\n",
    "sentence = MySentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for x in sentence:\n",
    "    word_counts.update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n",
      "[('the', 69971), (',', 58334), ('.', 49346), ('of', 36412), ('and', 28853)]\n",
      "[('aviary', 1), ('olive-flushed', 1), ('coral-colored', 1), ('boucle', 1), ('stupefying', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counts))\n",
    "print(word_counts.most_common(5))\n",
    "print(word_counts.most_common()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13891\n",
      "[('one', 3504), ('would', 2719), ('said', 1961), ('time', 1695), ('new', 1646)]\n",
      "[('furrow', 5), ('richert', 5), ('kafka', 5), ('poitrine', 5), ('quasimodo', 5)]\n"
     ]
    }
   ],
   "source": [
    "# minimum word count = 5\n",
    "word_counts2 = Counter()\n",
    "for key,value in word_counts.items():\n",
    "    if value >= 5:\n",
    "        word_counts2[key] = value\n",
    "print(len(word_counts2))\n",
    "print(word_counts2.most_common(5))\n",
    "print(word_counts2.most_common()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504865\n",
      "[(('of', 'the'), 20851), (('the', 'of'), 20851), (('the', ','), 11580), ((',', 'the'), 11580), ((',', 'and'), 9296)]\n"
     ]
    }
   ],
   "source": [
    "# count co-occurance\n",
    "joint = Counter()\n",
    "for isen in sentence:\n",
    "    for j, word in enumerate(isen):\n",
    "        if word not in word_counts:\n",
    "            continue\n",
    "        index_min = max(0, j-iwin)\n",
    "        index_max = min(len(isen), j+iwin+1)\n",
    "        index = [ii for ii in range(index_min, index_max) if ii!=j]\n",
    "        for iin in index:\n",
    "            #if isen[iin] in word_counts:\n",
    "            joint[(word, isen[iin])] += 1\n",
    "print(len(joint))\n",
    "print(joint.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts2['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate index for words in vocabulary\n",
    "index_dict = dict()\n",
    "word_dict = dict()\n",
    "ii = 0\n",
    "for i in word_counts:\n",
    "    index_dict[i] = ii\n",
    "    word_dict[ii] = i\n",
    "    ii += 1\n",
    "    \n",
    "# transform count joint to sparse matrix\n",
    "import math\n",
    "row_index = []\n",
    "col_index = []\n",
    "values = []\n",
    "for (wi,wj), count in joint.items():\n",
    "    row = index_dict[wi]\n",
    "    col = index_dict[wj]\n",
    "    prod = word_counts[wi]*word_counts[wj]*len(joint)/(len(word_counts)**2)\n",
    "    if count > prod:\n",
    "        value = math.log(count/prod)\n",
    "    else:\n",
    "        value = 0\n",
    "    row_index.append(row)\n",
    "    col_index.append(col)\n",
    "    values.append(value)\n",
    "\n",
    "from scipy import sparse\n",
    "ppmi = sparse.csr_matrix((values, (row_index, col_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    1,     1,     1, ..., 49813, 49813, 49814], dtype=int32),\n",
       " array([    2,     3,    41, ...,  7178, 49812,  7178], dtype=int32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def ww_sim(word, mat, topn=10):\n",
    "    indx = index_dict[word]\n",
    "    if isinstance(mat, sparse.csr_matrix):\n",
    "        v1 = mat.getrow(indx)\n",
    "    else:\n",
    "        v1 = mat[indx:indx+1, :]\n",
    "    sims = cosine_similarity(mat, v1).flatten()\n",
    "    sindxs = np.argsort(-sims)\n",
    "#     print(sindxs[0:topn])\n",
    "#     print(index_dict[9254])\n",
    "#     print(sims[9254])\n",
    "    sim_word_scores = [(word_dict[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n",
    "    return sim_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uncle', 0.9999999999999998),\n",
       " (\"stowe's\", 0.2231759231137599),\n",
       " ('lyman', 0.18067858267842332),\n",
       " ('chuckled', 0.17889973368974996),\n",
       " ('harriet', 0.17816836465527214),\n",
       " ('morse', 0.17554553236411702),\n",
       " (\"tom's\", 0.170997917706415),\n",
       " ('underlay', 0.1659886873587491),\n",
       " ('beecher', 0.14489940818841424),\n",
       " ('shortest', 0.1431663593894243)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim('uncle', ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49815, 100) (100,) (100, 49815)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import linalg\n",
    "uu,ss,vv = linalg.svds(ppmi, idim)\n",
    "print(np.shape(uu),np.shape(ss),np.shape(vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/Users/mac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "unorm = uu / np.sqrt(np.sum(uu*uu, axis=1, keepdims=True))\n",
    "vnorm = vv / np.sqrt(np.sum(vv*vv, axis=0, keepdims=True))\n",
    "#word_vecs = unorm\n",
    "#word_vecs = vnorm.T\n",
    "word_vecs = uu + vv.T\n",
    "word_vecs_norm = word_vecs / np.sqrt(np.sum(word_vecs*word_vecs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 1.0000000000000002),\n",
       " ('19-foot', 0.9980361675974996),\n",
       " ('vocalists', 0.9965653314649788),\n",
       " ('bumblebee', 0.9599715105301654),\n",
       " ('parasite', 0.9287603772810248),\n",
       " ('andrena', 0.9229003957172038),\n",
       " ('dissenters', 0.9217030872903869),\n",
       " ('cowbirds', 0.9069719691806799),\n",
       " ('spate', 0.8845950933788866),\n",
       " ('bobbed', 0.8840057140235706)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim(\"female\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strike', 0.9999999999999998),\n",
       " ('shrinking', 0.9156697744209977),\n",
       " ('heaven', 0.8904639794587017),\n",
       " ('slang', 0.8888747215148729),\n",
       " (\"t'ien\", 0.8875081627802461),\n",
       " (\"smugglers'\", 0.8875081627802459),\n",
       " ('tollgate', 0.8875081627802458),\n",
       " ('unnerving', 0.8875081627802456),\n",
       " ('haint', 0.8872022500803394),\n",
       " ('piously', 0.8799832390289005)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim(\"strike\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uncle', 1.0000000000000004),\n",
       " ('remus', 0.9993300025849559),\n",
       " (\"elaine's\", 0.9993300025849557),\n",
       " ('ffortescue', 0.9993300025849556),\n",
       " (\"dan'l\", 0.9985238437324462),\n",
       " ('dragooned', 0.993848203422079),\n",
       " ('chuckled', 0.9910543209000767),\n",
       " ('randolph', 0.9852530055504984),\n",
       " ('izaak', 0.9770375458670217),\n",
       " (\"tom's\", 0.9746761864136466)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww_sim(\"uncle\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sentences iterable can be simply a list, but for larger corpora, consider a generator that streams the sentences directly from disk/network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t0.0\n",
      "  (0, 7)\t0.0\n",
      "  (0, 29)\t0.0\n",
      "  (0, 30)\t0.0\n",
      "  (0, 31)\t0.0\n",
      "  (0, 58)\t0.0\n",
      "  (0, 59)\t0.0\n",
      "  (0, 60)\t0.0\n",
      "  (0, 64)\t0.0\n",
      "  (0, 66)\t0.0\n",
      "  (0, 67)\t0.0\n",
      "  (0, 90)\t0.0\n",
      "  (0, 106)\t0.0\n",
      "  (0, 119)\t0.0\n",
      "  (0, 129)\t0.0\n",
      "  (0, 130)\t0.0\n",
      "  (0, 131)\t0.0\n",
      "  (0, 138)\t0.0\n",
      "  (0, 139)\t0.0\n",
      "  (0, 149)\t0.0\n",
      "  (0, 150)\t0.0\n",
      "  (0, 151)\t0.0\n",
      "  (0, 156)\t0.0\n",
      "  (0, 157)\t0.0\n",
      "  (0, 158)\t0.0\n",
      "  (0, 210)\t0.0\n",
      "  (0, 211)\t0.0\n",
      "  (0, 215)\t0.0\n",
      "  (0, 222)\t0.0\n",
      "  (0, 235)\t0.0\n",
      "  (0, 236)\t0.0\n",
      "  (0, 237)\t0.0\n",
      "  (0, 241)\t0.6394438532335271\n",
      "  (0, 242)\t0.0\n",
      "  (0, 243)\t0.0\n",
      "  (0, 244)\t0.0\n",
      "  (0, 246)\t0.6394438532335271\n",
      "  (0, 252)\t0.0\n",
      "  (0, 304)\t0.0\n",
      "  (0, 1051)\t0.0\n",
      "  (0, 1212)\t0.0\n",
      "  (0, 2258)\t0.0\n",
      "  (0, 4037)\t0.0\n",
      "  (0, 4165)\t0.0\n",
      "  (0, 6212)\t0.0\n",
      "  (0, 6789)\t0.0\n"
     ]
    }
   ],
   "source": [
    "print(ppmi[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-d30a2fe9d97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../NLPdata/hw3/savedModel/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "word_vecs.save(\"../../NLPdata/hw3/savedModel/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13891, 100)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.63849412e-04, -9.70924308e-06, -3.89196601e-04, -1.99606045e-04,\n",
       "       -1.52572737e-04,  3.92952168e-04,  4.11227720e-16,  6.34080014e-04,\n",
       "       -8.64653808e-05, -1.56360244e-03, -5.49293412e-15, -2.56142763e-17,\n",
       "        1.16559588e-04, -4.07202049e-05,  4.32493179e-05,  2.21068619e-04,\n",
       "        3.71345924e-04,  4.73482216e-04,  8.14642407e-17, -4.92336396e-04,\n",
       "       -1.26622883e-04,  1.42008801e-15,  6.22602646e-04,  9.23833035e-05,\n",
       "       -1.18927200e-04,  9.85675300e-17,  2.16759825e-05, -4.91414635e-16,\n",
       "       -3.35825277e-04, -3.18863859e-16, -1.83852395e-04, -8.15787470e-05,\n",
       "        2.11504127e-17, -8.56977114e-17, -6.59427894e-05,  2.85409917e-04,\n",
       "       -1.09294355e-16,  6.32557204e-05,  5.77771367e-05,  8.97643609e-05,\n",
       "       -4.61098228e-05,  2.48408268e-05, -3.19926818e-04, -9.76574519e-05,\n",
       "       -5.61873316e-05,  6.93308960e-04,  3.10089953e-15, -1.51924772e-04,\n",
       "       -2.84483907e-05,  1.10950504e-04, -5.57999895e-17,  1.62489884e-05,\n",
       "       -1.04200530e-04, -5.91413011e-05,  3.16817047e-04,  5.76590544e-05,\n",
       "        1.64330426e-06, -5.69516610e-04,  9.03169577e-06, -1.88214933e-05,\n",
       "       -1.12542091e-04, -1.75870998e-04, -9.78829467e-05,  7.43822831e-05,\n",
       "        2.59406076e-04, -3.80664763e-04,  1.97051600e-04, -3.02960186e-05,\n",
       "        2.94307443e-05, -1.90648587e-04, -5.88568542e-05, -1.07930207e-04,\n",
       "       -4.16241997e-04,  4.30507776e-05,  1.45312311e-04,  1.95227415e-04,\n",
       "        4.36293493e-03, -4.09497445e-05, -1.27053849e-05, -1.73563958e-04,\n",
       "       -2.35633146e-04, -8.45838896e-05,  3.96022024e-05,  4.14037738e-05,\n",
       "        1.62074476e-05, -7.53819548e-06,  6.75414849e-05,  1.29570872e-06,\n",
       "       -3.44006714e-05,  5.28233248e-05, -2.69400607e-05,  2.61495000e-04,\n",
       "        8.93578253e-06,  6.34366818e-06, -6.09949907e-06,  7.08059894e-05,\n",
       "        3.08302097e-05, -9.56924279e-07,  3.64591979e-07,  2.60713917e-06])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
