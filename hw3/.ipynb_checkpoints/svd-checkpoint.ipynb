{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import process\n",
    "text = 'data/brown.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import casual\n",
    "\n",
    "import json\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import numpy as np\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from process import load_model, load_msr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowList = [2,5,10]\n",
    "dimList = [100,300,1000]\n",
    "EPOCHS = 5\n",
    "iwin = 2\n",
    "idim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, 'r'):\n",
    "            yield [word.lower() for word in tokenizer.tokenize(line) if word.lower() not in stop_words]\n",
    "sentence = MySentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for x in sentence:\n",
    "    word_counts.update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42281\n",
      "[('one', 3504), ('would', 2719), ('said', 1961), ('time', 1695), ('new', 1646)]\n",
      "[('perelman', 1), ('exhaling', 1), ('aviary', 1), ('boucle', 1), ('stupefying', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counts))\n",
    "print(word_counts.most_common(5))\n",
    "print(word_counts.most_common()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13891\n",
      "[('one', 3504), ('would', 2719), ('said', 1961), ('time', 1695), ('new', 1646)]\n",
      "[('furrow', 5), ('richert', 5), ('kafka', 5), ('poitrine', 5), ('quasimodo', 5)]\n"
     ]
    }
   ],
   "source": [
    "# minimum word count = 5\n",
    "word_counts2 = Counter()\n",
    "for key,value in word_counts.items():\n",
    "    if value >= 5:\n",
    "        word_counts2[key] = value\n",
    "print(len(word_counts2))\n",
    "print(word_counts2.most_common(5))\n",
    "print(word_counts2.most_common()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197681\n",
      "[(('united', 'states'), 401), (('states', 'united'), 401), (('af', 'af'), 392), (('new', 'york'), 312), (('york', 'new'), 312)]\n"
     ]
    }
   ],
   "source": [
    "# count co-occurance\n",
    "joint = Counter()\n",
    "for isen in sentence:\n",
    "    for j, word in enumerate(isen):\n",
    "        if word not in word_counts2:\n",
    "            continue\n",
    "        index_min = max(0, j-iwin)\n",
    "        index_max = min(len(isen), j+iwin+1)\n",
    "        index = [ii for ii in range(index_min, index_max) if ii!=j]\n",
    "        for iin in index:\n",
    "            if isen[iin] in word_counts2:\n",
    "                joint[(word, isen[iin])] += 1\n",
    "print(len(joint))\n",
    "print(joint.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts2['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate index for words in vocabulary\n",
    "index_dict = dict()\n",
    "ii = 0\n",
    "for i in word_counts2:\n",
    "    index_dict[i] = ii\n",
    "    ii += 1\n",
    "    \n",
    "import math\n",
    "row_index = []\n",
    "col_index = []\n",
    "values = []\n",
    "for (wi,wj), count in joint.items():\n",
    "    row = index_dict[wi]\n",
    "    col = index_dict[wj]\n",
    "    prod = word_counts2[wi]*word_counts2[wj]*len(joint)/(len(word_counts2)**2)\n",
    "    if count > prod:\n",
    "        value = math.log(count/prod)\n",
    "    else:\n",
    "        value = 0\n",
    "    row_index.append(row)\n",
    "    col_index.append(col)\n",
    "    values.append(value)\n",
    "\n",
    "from scipy import sparse\n",
    "ppmi = sparse.csr_matrix((values, (row_index, col_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     7, ..., 13886, 13888, 13890], dtype=int32),\n",
       " array([  241,   246,  3591, ..., 13885, 12761, 10634], dtype=int32))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13891 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 48 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi.getrow(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sentences iterable can be simply a list, but for larger corpora, consider a generator that streams the sentences directly from disk/network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t0.0\n",
      "  (0, 7)\t0.0\n",
      "  (0, 29)\t0.0\n",
      "  (0, 30)\t0.0\n",
      "  (0, 31)\t0.0\n",
      "  (0, 58)\t0.0\n",
      "  (0, 59)\t0.0\n",
      "  (0, 60)\t0.0\n",
      "  (0, 64)\t0.0\n",
      "  (0, 66)\t0.0\n",
      "  (0, 67)\t0.0\n",
      "  (0, 90)\t0.0\n",
      "  (0, 106)\t0.0\n",
      "  (0, 119)\t0.0\n",
      "  (0, 129)\t0.0\n",
      "  (0, 130)\t0.0\n",
      "  (0, 131)\t0.0\n",
      "  (0, 138)\t0.0\n",
      "  (0, 139)\t0.0\n",
      "  (0, 149)\t0.0\n",
      "  (0, 150)\t0.0\n",
      "  (0, 151)\t0.0\n",
      "  (0, 156)\t0.0\n",
      "  (0, 157)\t0.0\n",
      "  (0, 158)\t0.0\n",
      "  (0, 210)\t0.0\n",
      "  (0, 211)\t0.0\n",
      "  (0, 215)\t0.0\n",
      "  (0, 222)\t0.0\n",
      "  (0, 235)\t0.0\n",
      "  (0, 236)\t0.0\n",
      "  (0, 237)\t0.0\n",
      "  (0, 241)\t0.6394438532335271\n",
      "  (0, 242)\t0.0\n",
      "  (0, 243)\t0.0\n",
      "  (0, 244)\t0.0\n",
      "  (0, 246)\t0.6394438532335271\n",
      "  (0, 252)\t0.0\n",
      "  (0, 304)\t0.0\n",
      "  (0, 1051)\t0.0\n",
      "  (0, 1212)\t0.0\n",
      "  (0, 2258)\t0.0\n",
      "  (0, 4037)\t0.0\n",
      "  (0, 4165)\t0.0\n",
      "  (0, 6212)\t0.0\n",
      "  (0, 6789)\t0.0\n"
     ]
    }
   ],
   "source": [
    "print(ppmi[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
