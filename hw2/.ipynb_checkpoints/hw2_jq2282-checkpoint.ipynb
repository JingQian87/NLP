{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run hw2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Imports - our files\n",
    "import utils\n",
    "import models\n",
    "\n",
    "# Global definitions - data\n",
    "DATA_FN = 'data/crowdflower_data.csv'\n",
    "LABEL_NAMES = [\"happiness\", \"worry\", \"neutral\", \"sadness\"]\n",
    "\n",
    "# Global definitions - architecture\n",
    "EMBEDDING_DIM = 100  # We will use pretrained 100-dimensional GloVe\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "USE_CUDA = torch.cuda.is_available()  # CUDA will be available if you are using the GPU image for this homework\n",
    "\n",
    "# Global definitions - saving and loading data\n",
    "FRESH_START = True  # set this to false after running once with True to just load your preprocessed data from file\n",
    "#                     (good for debugging)\n",
    "TEMP_FILE = \"temporary_data.pkl\"  # if you set FRESH_START to false, the program will look here for your data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataLoaders and embeddings from file....\n"
     ]
    }
   ],
   "source": [
    "# load the data and embeddings from file\n",
    "try:\n",
    "    with open(TEMP_FILE, \"rb\") as f:\n",
    "        print(\"Loading DataLoaders and embeddings from file....\")\n",
    "        train_generator, dev_generator, test_generator, embeddings, train_data = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You need to have saved your data with FRESH_START=True once in order to load it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 91, 100])\n",
      "torch.Size([128, 100])\n"
     ]
    }
   ],
   "source": [
    "# test embeddings\n",
    "# ebs = nn.Embedding.from_pretrained(embeddings)\n",
    "# train1 = ebs(train_batch)\n",
    "# print(np.shape(train1))\n",
    "# train11 = torch.sum(train1,dim=1)\n",
    "# print(np.shape(train11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNetwork(\n",
      "  (embedding): Embedding(17635, 100)\n",
      "  (dense1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (dense2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim, hidden_dim, weight):\n",
    "        super(DenseNetwork, self).__init__()\n",
    "\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Here, create any layers and attributes your network needs.\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.dense1 = nn.Linear(embed_dim, hidden_dim) \n",
    "        self.dense2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()     \n",
    "\n",
    "    def forward(self, x):\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Fill in the forward pass of your neural network.\n",
    "        # TODO: (The backward pass will be performed by PyTorch magic for you!)\n",
    "        # TODO: Your architecture should...\n",
    "        # TODO: 1) Put the words through an Embedding layer (which was initialized with the pretrained embeddings);\n",
    "        x = self.embedding(x)\n",
    "        # TODO: 2) Take the sum of all word embeddings in a sentence\n",
    "        x = torch.sum(x,dim=1).float()\n",
    "        # TODO: 3) Feed the result into 2-layer feedforward network which produces a 4-vector of values,\n",
    "        # TODO: one for each class\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "        \n",
    "net = DenseNetwork(EMBEDDING_DIM, NUM_CLASSES, 64, embeddings)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(net(train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(27.9350, grad_fn=<AddBackward0>)\n",
      "1 tensor(27.3739, grad_fn=<AddBackward0>)\n",
      "2 tensor(27.0554, grad_fn=<AddBackward0>)\n",
      "3 tensor(26.8350, grad_fn=<AddBackward0>)\n",
      "4 tensor(26.5253, grad_fn=<AddBackward0>)\n",
      "5 tensor(25.8699, grad_fn=<AddBackward0>)\n",
      "6 tensor(25.6448, grad_fn=<AddBackward0>)\n",
      "7 tensor(25.5283, grad_fn=<AddBackward0>)\n",
      "8 tensor(25.5029, grad_fn=<AddBackward0>)\n",
      "9 tensor(25.4591, grad_fn=<AddBackward0>)\n",
      "10 tensor(25.4942, grad_fn=<AddBackward0>)\n",
      "11 tensor(25.4476, grad_fn=<AddBackward0>)\n",
      "12 tensor(25.4547, grad_fn=<AddBackward0>)\n",
      "13 tensor(25.4571, grad_fn=<AddBackward0>)\n",
      "14 tensor(25.4687, grad_fn=<AddBackward0>)\n",
      "15 tensor(25.4762, grad_fn=<AddBackward0>)\n",
      "16 tensor(25.4888, grad_fn=<AddBackward0>)\n",
      "17 tensor(25.4845, grad_fn=<AddBackward0>)\n",
      "18 tensor(25.6559, grad_fn=<AddBackward0>)\n",
      "19 tensor(25.6036, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-e178bbf84c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "model = DenseNetwork(EMBEDDING_DIM, NUM_CLASSES, HIDDEN_DIM, embeddings)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "  \n",
    "EPOCHS = 20\n",
    "losses = []\n",
    "for iepoch in range(EPOCHS): \n",
    "    for train_batch, train_label in train_generator:\n",
    "        # Compute and print loss\n",
    "        loss = criterion(model(train_batch),train_label)\n",
    "        #print(loss.item()) \n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss = 0\n",
    "    for ibatch, ilabel in dev_generator:\n",
    "        dev_loss = criterion(model(ibatch), ilabel)\n",
    "        total_loss += dev_loss\n",
    "    print(iepoch, total_loss)\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(1,EPOCHS+1)\n",
    "plt.plot(x, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17635, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x106df65f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, train_generator, dev_generator):\n",
    "    \"\"\"\n",
    "    Perform the actual training of the model based on the train and dev sets.\n",
    "    :param model: one of your models, to be trained to perform 4-way emotion classification\n",
    "    :param loss_fn: a function that can calculate loss between the predicted and gold labels\n",
    "    :param optimizer: a created optimizer you will use to update your model weights\n",
    "    :param train_generator: a DataLoader that provides batches of the training set\n",
    "    :param dev_generator: a DataLoader that provides batches of the development set\n",
    "    :return model, the trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for epoch in range(50):\n",
    "        # Forward Propagation\n",
    "        y_pred = model(x)\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentNetwork(\n",
      "  (embedding): Embedding(17635, 100)\n",
      "  (lstm): LSTM(100, 64, num_layers=2)\n",
      "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "class RecurrentNetwork(nn.Module):\n",
    "    def __init__(self, sentence_len, output_dim, hidden_dim, weight):\n",
    "        super(RecurrentNetwork, self).__init__()\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Here, create any layers and attributes your network needs.\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.sentence_len = sentence_len\n",
    "        self.embed_dim = np.shape(weight)[1]  \n",
    "        # Define hidden dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define 2-layer LSTM\n",
    "        self.lstm = nn.LSTM(self.embed_dim, self.hidden_dim,num_layers=2) \n",
    "        #self.lstm2 = nn.LSTM(hidden_dim, output_dim)\n",
    "        # Define final transform layer\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "        \n",
    "    # x is a PaddedSequence for an RNN\n",
    "    def forward(self, x):\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Fill in the forward pass of your neural network.\n",
    "        # TODO: (The backward pass will be performed by PyTorch magic for you!)\n",
    "        # TODO: Your architecture should...\n",
    "        # TODO: 1) Put the words through an Embedding layer (which was initialized with the pretrained embeddings);\n",
    "        batch_size = np.shape(x)[0]\n",
    "        if np.shape(x)[1] != self.sentence_len:\n",
    "            x = self.pad(x)\n",
    "        mid = self.embedding(x).float()\n",
    "        # TODO: 2) Feed the sequence of embeddings through a 2-layer RNN\n",
    "        mid = mid.view(self.sentence_len, batch_size, -1)\n",
    "        # Add hidden layer\n",
    "        self.hidden = self.initialize_hidden(x)\n",
    "        out, _ = self.lstm(mid, self.hidden)\n",
    "        # TODO: 3) Feed the last output state into a dense layer to become a 4-vector of values, one for each class    \n",
    "        out = self.linear(out[-1].view(batch_size, -1))\n",
    "        return self.relu(out)\n",
    "        \n",
    "    def initialize_hidden(self,x):\n",
    "        # n_layers * n_directions, batch_size, rnn_hidden_size\n",
    "        batch_size = np.shape(x)[0]\n",
    "        return (autograd.Variable(torch.zeros(2, batch_size, self.hidden_dim)),\n",
    "               autograd.Variable(torch.zeros(2, batch_size, self.hidden_dim)))\n",
    "    \n",
    "    def pad(self, x):\n",
    "        if np.shape(x)[1] > self.sentence_len:\n",
    "            return x[:,:self.sentence_len]\n",
    "        elif np.shape(x)[1] < self.sentence_len:\n",
    "            tmp = torch.zeros(np.shape(x)[0], self.sentence_len-np.shape(x)[1], dtype=torch.long)\n",
    "            return torch.cat((x,tmp), 1)\n",
    "        \n",
    "net = RecurrentNetwork(91, NUM_CLASSES, 64, embeddings)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(29.9603, grad_fn=<AddBackward0>)\n",
      "1 tensor(29.9647, grad_fn=<AddBackward0>)\n",
      "2 tensor(29.9727, grad_fn=<AddBackward0>)\n",
      "3 tensor(29.9999, grad_fn=<AddBackward0>)\n",
      "4 tensor(30.0317, grad_fn=<AddBackward0>)\n",
      "5 tensor(30.0457, grad_fn=<AddBackward0>)\n",
      "6 tensor(30.0704, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "SENTENCE_LEN = 91\n",
    "model2 = RecurrentNetwork(SENTENCE_LEN, NUM_CLASSES, HIDDEN_DIM, embeddings)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "  \n",
    "EPOCHS = 20\n",
    "losses = []\n",
    "for iepoch in range(EPOCHS): \n",
    "    for train_batch, train_label in train_generator:\n",
    "        # Compute and print loss\n",
    "        loss = criterion(model2(train_batch),train_label)\n",
    "        #print(loss.item()) \n",
    "\n",
    "        # Zero the gradients\n",
    "        model2.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss = 0\n",
    "    for ibatch, ilabel in dev_generator:\n",
    "        dev_loss = criterion(model2(ibatch), ilabel)\n",
    "        total_loss += dev_loss\n",
    "    print(iepoch, total_loss)\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: \n",
      "tensor([25.9427])\n",
      "F-score: \n",
      "0.45296065571667343\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, loss_fn, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model on the development set, providing the loss and macro F1 score.\n",
    "    :param model: a model that performs 4-way emotion classification\n",
    "    :param loss_fn: a function that can calculate loss between the predicted and gold labels\n",
    "    :param test_generator: a DataLoader that provides batches of the testing set\n",
    "    \"\"\"\n",
    "    gold = []\n",
    "    predicted = []\n",
    "\n",
    "    # Keep track of the loss\n",
    "    loss = torch.zeros(1)  # requires_grad = False by default; float32 by default\n",
    "    if USE_CUDA:\n",
    "        loss = loss.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over batches in the test dataset\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in test_generator:\n",
    "            # Predict\n",
    "            y_pred = model(X_b)\n",
    "\n",
    "            # Save gold and predicted labels for F1 score - take the argmax to convert to class labels\n",
    "            gold.extend(y_b.cpu().detach().numpy())\n",
    "            predicted.extend(y_pred.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "            loss += loss_fn(y_pred.double(), y_b.long()).data\n",
    "\n",
    "    # Print total loss and macro F1 score\n",
    "    print(\"Test loss: \")\n",
    "    print(loss)\n",
    "    print(\"F-score: \")\n",
    "    print(f1_score(gold, predicted, average='macro'))\n",
    "\n",
    "test_model(model, criterion, test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
