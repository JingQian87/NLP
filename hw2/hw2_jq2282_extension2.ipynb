{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Imports - our files\n",
    "import utils\n",
    "import models\n",
    "\n",
    "# Global definitions - data\n",
    "DATA_FN = 'data/crowdflower_data.csv'\n",
    "LABEL_NAMES = [\"happiness\", \"worry\", \"neutral\", \"sadness\"]\n",
    "\n",
    "# Global definitions - architecture\n",
    "EMBEDDING_DIM = 100  # We will use pretrained 100-dimensional GloVe\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "USE_CUDA = torch.cuda.is_available()  # CUDA will be available if you are using the GPU image for this homework\n",
    "TEMP_FILE = \"temporary_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataLoaders and embeddings from file....\n"
     ]
    }
   ],
   "source": [
    "# load the data and embeddings from file\n",
    "try:\n",
    "    with open(TEMP_FILE, \"rb\") as f:\n",
    "        print(\"Loading DataLoaders and embeddings from file....\")\n",
    "        train_generator, dev_generator, test_generator, embeddings, train_data = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You need to have saved your data with FRESH_START=True once in order to load it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNetwork(\n",
      "  (embedding): Embedding(17635, 100)\n",
      "  (dense1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (dense2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim, hidden_dim, weight):\n",
    "        super(DenseNetwork, self).__init__()\n",
    "\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Here, create any layers and attributes your network needs.\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.dense1 = nn.Linear(embed_dim, hidden_dim) \n",
    "        self.dense2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()     \n",
    "\n",
    "    def forward(self, x):\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Fill in the forward pass of your neural network.\n",
    "        # TODO: (The backward pass will be performed by PyTorch magic for you!)\n",
    "        # TODO: Your architecture should...\n",
    "        # TODO: 1) Put the words through an Embedding layer (which was initialized with the pretrained embeddings);\n",
    "        x = self.embedding(x)\n",
    "        # TODO: 2) Take the sum of all word embeddings in a sentence\n",
    "        x = torch.sum(x,dim=1).float()\n",
    "        # TODO: 3) Feed the result into 2-layer feedforward network which produces a 4-vector of values,\n",
    "        # TODO: one for each class\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "        \n",
    "net = DenseNetwork(EMBEDDING_DIM, NUM_CLASSES, 64, embeddings)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(26.8893, grad_fn=<AddBackward0>)\n",
      "1 tensor(26.2983, grad_fn=<AddBackward0>)\n",
      "2 tensor(25.9858, grad_fn=<AddBackward0>)\n",
      "3 tensor(25.8591, grad_fn=<AddBackward0>)\n",
      "4 tensor(25.7640, grad_fn=<AddBackward0>)\n",
      "5 tensor(25.6948, grad_fn=<AddBackward0>)\n",
      "6 tensor(25.6656, grad_fn=<AddBackward0>)\n",
      "7 tensor(25.6230, grad_fn=<AddBackward0>)\n",
      "8 tensor(25.6038, grad_fn=<AddBackward0>)\n",
      "9 tensor(25.6043, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "model = DenseNetwork(EMBEDDING_DIM, NUM_CLASSES, HIDDEN_DIM, embeddings)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "  \n",
    "EPOCHS = 20\n",
    "losses = []\n",
    "for iepoch in range(EPOCHS): \n",
    "    for train_batch, train_label in train_generator:\n",
    "        # Compute the loss\n",
    "        loss = criterion(model(train_batch),train_label)\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss = 0\n",
    "    for ibatch, ilabel in dev_generator:\n",
    "        dev_loss = criterion(model(ibatch), ilabel)\n",
    "        total_loss += dev_loss\n",
    "    print(iepoch, total_loss)\n",
    "    losses.append(total_loss)\n",
    "    if iepoch > 1 and losses[-2]-total_loss < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: \n",
      "tensor([25.9882])\n",
      "F-score: \n",
      "0.42962728871157085\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, loss_fn, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model on the development set, providing the loss and macro F1 score.\n",
    "    :param model: a model that performs 4-way emotion classification\n",
    "    :param loss_fn: a function that can calculate loss between the predicted and gold labels\n",
    "    :param test_generator: a DataLoader that provides batches of the testing set\n",
    "    \"\"\"\n",
    "    gold = []\n",
    "    predicted = []\n",
    "\n",
    "    # Keep track of the loss\n",
    "    loss = torch.zeros(1)  # requires_grad = False by default; float32 by default\n",
    "    if USE_CUDA:\n",
    "        loss = loss.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over batches in the test dataset\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in test_generator:\n",
    "            # Predict\n",
    "            y_pred = model(X_b)\n",
    "\n",
    "            # Save gold and predicted labels for F1 score - take the argmax to convert to class labels\n",
    "            gold.extend(y_b.cpu().detach().numpy())\n",
    "            predicted.extend(y_pred.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "            loss += loss_fn(y_pred.double(), y_b.long()).data\n",
    "\n",
    "    # Print total loss and macro F1 score\n",
    "    print(\"Test loss: \")\n",
    "    print(loss)\n",
    "    print(\"F-score: \")\n",
    "    print(f1_score(gold, predicted, average='macro'))\n",
    "\n",
    "test_model(model, criterion, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNetwork(\n",
      "  (embedding): Embedding(17635, 100)\n",
      "  (dense1): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (dense2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim, hidden_dim, weight):\n",
    "        super(DenseNetwork, self).__init__()\n",
    "\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Here, create any layers and attributes your network needs.\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.dense1 = nn.Linear(embed_dim, hidden_dim) \n",
    "        self.dense2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()     \n",
    "\n",
    "    def get_len(self, x):\n",
    "        x_len = []\n",
    "        for ix in x:\n",
    "            if ix[-1] != 0:\n",
    "                x_len.append(len(ix)*1.0)\n",
    "            else:\n",
    "                x_len.append((ix==0).nonzero()[0])           \n",
    "        return x_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # TODO: Fill in the forward pass of your neural network.\n",
    "        # TODO: (The backward pass will be performed by PyTorch magic for you!)\n",
    "        # TODO: Your architecture should...\n",
    "        # TODO: 1) Put the words through an Embedding layer (which was initialized with the pretrained embeddings);\n",
    "        x_lengths = self.get_len(x)\n",
    "        x = self.embedding(x)\n",
    "        # TODO: 2) Take the average of all non-zero word embeddings in a sentence\n",
    "        y = torch.zeros(x.size(0), x.size(2), dtype=torch.float)\n",
    "        for i in range(x.size(0)):\n",
    "            for j in range(x.size(2)):\n",
    "                y[i][j] = x[i,:,j].sum()/x_lengths[i]\n",
    "#         for i, l in enumerate(x_lengths):\n",
    "#             selected[i,:] = out[i,l-1,:]\n",
    "        # TODO: 3) Feed the result into 2-layer feedforward network which produces a 4-vector of values,\n",
    "        # TODO: one for each class\n",
    "        y = self.dense1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.dense2(y)\n",
    "        return y\n",
    "        \n",
    "net = DenseNetwork(EMBEDDING_DIM, NUM_CLASSES, 64, embeddings)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "0 tensor(29.8221, grad_fn=<AddBackward0>)\n",
      "yes\n",
      "1 tensor(29.6444, grad_fn=<AddBackward0>)\n",
      "yes\n",
      "2 tensor(29.5903, grad_fn=<AddBackward0>)\n",
      "yes\n",
      "3 tensor(29.5680, grad_fn=<AddBackward0>)\n",
      "yes\n",
      "4 tensor(29.5561, grad_fn=<AddBackward0>)\n",
      "yes\n",
      "5 tensor(29.5489, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "model = DenseNetwork(EMBEDDING_DIM, NUM_CLASSES, HIDDEN_DIM, embeddings)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "  \n",
    "EPOCHS = 20\n",
    "losses = []\n",
    "for iepoch in range(EPOCHS): \n",
    "    for train_batch, train_label in train_generator:\n",
    "        # Compute the loss\n",
    "        loss = criterion(model(train_batch),train_label)\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss = 0\n",
    "    print('yes')\n",
    "    for ibatch, ilabel in dev_generator:\n",
    "        dev_loss = criterion(model(ibatch), ilabel)\n",
    "        total_loss += dev_loss\n",
    "    print(iepoch, total_loss)\n",
    "    losses.append(total_loss)\n",
    "    if iepoch > 1 and losses[-2]-total_loss < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: \n",
      "tensor([29.4282])\n",
      "F-score: \n",
      "0.22087838416284739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, loss_fn, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model on the development set, providing the loss and macro F1 score.\n",
    "    :param model: a model that performs 4-way emotion classification\n",
    "    :param loss_fn: a function that can calculate loss between the predicted and gold labels\n",
    "    :param test_generator: a DataLoader that provides batches of the testing set\n",
    "    \"\"\"\n",
    "    gold = []\n",
    "    predicted = []\n",
    "\n",
    "    # Keep track of the loss\n",
    "    loss = torch.zeros(1)  # requires_grad = False by default; float32 by default\n",
    "    if USE_CUDA:\n",
    "        loss = loss.cuda()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over batches in the test dataset\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in test_generator:\n",
    "            # Predict\n",
    "            y_pred = model(X_b)\n",
    "\n",
    "            # Save gold and predicted labels for F1 score - take the argmax to convert to class labels\n",
    "            gold.extend(y_b.cpu().detach().numpy())\n",
    "            predicted.extend(y_pred.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "            loss += loss_fn(y_pred.double(), y_b.long()).data\n",
    "\n",
    "    # Print total loss and macro F1 score\n",
    "    print(\"Test loss: \")\n",
    "    print(loss)\n",
    "    print(\"F-score: \")\n",
    "    print(f1_score(gold, predicted, average='macro'))\n",
    "\n",
    "test_model(model, criterion, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(x):\n",
    "    x_len = []\n",
    "    for ix in x:\n",
    "        if ix[-1] != 0:\n",
    "            x_len.append(len(ix)*1.0)\n",
    "        else:\n",
    "            x_len.append((ix==0).nonzero()[0])           \n",
    "    return x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 91, 100])\n"
     ]
    }
   ],
   "source": [
    "np.shape(train_batch)\n",
    "x_len = get_len(train_batch)\n",
    "embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "x = embedding(train_batch)\n",
    "print(np.shape(x))\n",
    "y = torch.zeros(x.size(0), x.size(2), dtype=torch.float)\n",
    "\n",
    "for i in range(x.size(0)):\n",
    "    for j in range(x.size(2)):\n",
    "        #print(i,j)\n",
    "        y[i][j] = x[i,:,j].sum()/x_len[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.2256e-01,  2.1085e-01,  4.5296e-01,  1.8764e-01,  9.8591e-02,\n",
       "         1.8764e-01,  1.9244e-01,  1.8764e-01,  1.4220e-01,  1.8764e-01,\n",
       "        -2.0246e-01,  1.8764e-01,  1.8764e-01, -2.2716e-01,  1.8764e-01,\n",
       "         1.8764e-01,  8.2029e-02,  1.8764e-01,  1.8764e-01,  1.8764e-01,\n",
       "         2.3903e-01,  5.3169e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,\n",
       "         9.8591e-02,  1.8764e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,\n",
       "         1.8764e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,\n",
       "         1.8764e-01,  1.8764e-01, -6.6233e-04,  1.4220e-01,  1.8764e-01,\n",
       "         1.8764e-01,  1.8764e-01,  5.1249e-01,  1.8764e-01,  1.4220e-01,\n",
       "         1.8764e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01, -1.6737e-01,\n",
       "         1.8764e-01,  1.8764e-01,  9.8591e-02,  1.8764e-01,  9.8591e-02,\n",
       "         1.8764e-01,  2.0315e-01,  1.8764e-01,  1.8764e-01,  7.7639e-01,\n",
       "         1.8764e-01,  5.1249e-01,  1.8764e-01,  1.4220e-01,  1.8764e-01,\n",
       "         9.8591e-02,  1.8764e-01,  1.8764e-01,  5.1249e-01,  1.8764e-01,\n",
       "         1.8764e-01,  5.1249e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,\n",
       "         1.8764e-01,  1.8764e-01,  1.8764e-01,  4.5296e-01,  1.8764e-01,\n",
       "         5.1249e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,  1.8764e-01,\n",
       "         1.8764e-01,  5.1249e-01,  1.8764e-01,  1.8764e-01,  2.3903e-01,\n",
       "         1.8764e-01], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[40,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_len[40]#.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.3715), tensor(0.4451), tensor(-0.4311))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(x,dim=1).float()\n",
    "y[0][0],y[0][1],y[0][2]#/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
