{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP-Reading.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wUewM_F64xpJ","colab_type":"text"},"source":["# Reading 要求\n","* Lec1: Ch1 (done)\n","* Lec2: Ch6-6.2(done)\n","* Lec3: Ch2.1-2.6, 2.8(扫过，大致讲ML)\n","* Lec4: Ch4.4, 4.5 (done)\n","* Lec5: Ch3 (扫过，大致讲NN)\n","* Lec6: Ch14.2, 14.3(扫过)"]},{"cell_type":"code","metadata":{"id":"y46oDf2xFr9h","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1oyFgvpoFt9o","colab_type":"text"},"source":["## Chapter 1 Introduction\n","\n","1. NLP不同于一般ML的地方：\n","* NLP处理的文本是discrete的，因此不能gradually approach an optimal solution。\n","* NLP始终会有新建词汇，因此需要对train中没有的词汇robust。\n","* NLP是可组合的，可以创建任意长的短语。"]},{"cell_type":"markdown","metadata":{"id":"2pA9YZaOFuGN","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"EUDKpM8jFuKM","colab_type":"text"},"source":["## Chapter 6-6.2\n","For Sep 9 class: Language modeling\n"]},{"cell_type":"markdown","metadata":{"id":"LsHryMJNHO91","colab_type":"text"},"source":["#### 还没看书，课件lec2\n","Bigram model: Approximate $P(w_n|w_1^{n-1})$ by $P(w_n|w_{n-1})$\n","\n","N-gram model: $P(w_n|w_1^{n-1}) \\approx P(w_n|w_{n-N+1}^{n-1})$\n","\n","因此由Chain Rule: 对于bigram model: $P(w_1^n) \\approx \\prod_{k=1}^n P(w_k|w_{k-1})$.\n","\n","**Metric for the language model**: Perplexity and entropy\n","* Perplexity: $P(W) = P(w_1,w_2,...,w_N)^{-1/N}$ or $2^{-L(w)/M}$. 越小越好\n","\n","**Smoothing**少部分词汇频繁出现，大部分只出现一次。\n","\n","Zipf's law:词汇的出现频率大致正比于它在分布排名的倒数。\n","\n","Smoothing method:\n","* Add-one smoothing\n","* Backoff models\n","* Class-based smoothing\n","* Good-Turing"]},{"cell_type":"markdown","metadata":{"id":"xnG2qPsxOEll","colab_type":"text"},"source":[" $$\\textbf{f}(x) = [f_0(x), f_1(x), ... , f_7(x)]$$\n","where \n","\n","$$f_0(x) = \\begin{cases} 1 \\quad \\text{if } \\texttt{hi} \\in x\\\\ 0 \\quad \\text{otherwise} \\end{cases}$$\n","\n","$$f_1(x) = \\begin{cases} 1 \\quad \\text{if } \\texttt{hello} \\in x\\\\ 0 \\quad \\text{otherwise} \\end{cases}$$\n","\n","and so on for each unigram."]},{"cell_type":"markdown","metadata":{"id":"1Y8j-LbFaXyW","colab_type":"text"},"source":["## Chapter 4.4, 4.5\n","For Sep 16 class: Supervised ML\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q9y80tzVagz9","colab_type":"text"},"source":["### 4.4. Evaluating Classifiers\n","#### 1.\n","最简单的是**Accuracy**: $\\frac{1}{N}\\sum_i^N \\delta(y^{(i)} = \\hat{y})$\n","\n","#### 2.\n","问题是class imbalance. 解决措施：\n","* Build a balanced set. 问题是对极不平衡的，会浪费数据。\n","* 考虑Detection threshold.\n","\n","precision = TP/(TP+FP). 许多FP（假阳）会低precision.\\\\\n","recall = TP/(TP+FP). 许多FN（假阴）会低recall\n","\n","当假阳性比假阴性便宜时，优选高recall。例如，在对疾病症状进行初步筛查时，假阳性的成本可能是额外的测试，而假阴性则会导致疾病。没有得到治疗。相反，当假阳更昂贵时，首选高precision分类器：例如，在垃圾邮件检测中，假阴性是相对较小的不便，而误报可能意味着重要信息未读。\n","\n","$F-MEASURE = \\frac{2rp}{r+p}$.又称为$F_1$.推广到$F_{\\beta} = \\frac{(1+\\beta^2)rp}{\\beta^2p+r}$，其中$\\beta$是可调参数。\n","\n","\n","#### 3.Multi-class classification\n","每个class用一个F，再平均：\n","$Macro-F = \\frac{1}{K}\\sum_{k\\in K}F_k$. \n","在具有不平衡类分布的多类问题中，Macro F-MEASURE是分类器识别每个类的程度的平衡度量。在Micro F-MEASURE中，我们计算每个类的真阳性，误报和假阴性，然后将它们相加以计算单个回忆，精确度和F-MEASURE。该度量在实例而不是类之间进行平衡，因此它按照其频率对每个类进行加权, 与Macro F-MEASURE不同，后者对每个类进行相等的加权。\n","\n","#### 4. receiver operating characteristic (ROC) curve\n","X轴是precision, y轴是recall. 完美的分类器\n","在没有任何FP的情况下获得完美的recall，追踪从原点（0,0）到左上角（0,1），然后到（1,1）的“曲线”。在期望中，非判别分类器跟踪从原点（0,0）到右上角（1,1）的对角线。真正的分类器倾向于介于这两个极端之间。\n","\n","**area under the curve (AUC)**\n","AUC可以被解释为随机选择的正例将被分类器分配比随机选择的负例更高的概率。 完美的分类器具有AUC = 1; 非判别分类器的AUC = 0.5; 完全错误的分类器将具有AUC = 0。 与F-MEASURE相比，AUC的一个优点是0.5的基线不依赖于label distribution。"]},{"cell_type":"markdown","metadata":{"id":"h1ddxHlPz-Ty","colab_type":"text"},"source":["#### 5. 比较Classifier\n","NLP涉及比较不同的classifier，或者不同的feature sets.\n","\n","**Ablation testing involves systematically removing (ablating) various aspects of the classifier, such as feature groups, and testing the null hypothesis that the ablated classifier is as good as the full model.**\n","\n","不知道这个test怎么翻译。但是是看两种classifier准确性是否统计显著。\n","\n","**binomial test再看看？就是说c1和c2有N个不同的结果，其中有k个c1是对的，那么k/N的期望是1/2,也就是binomial的$\\theta$.还有什么single tail 和双边的。**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E1sUzk87102r","colab_type":"text"},"source":["#### 6.星号内容，random和multiple comparisons.\n","random是测量F measure. multiclass的讲到bonferroni（另一种方法叫false discovery rate FDR）\n"]},{"cell_type":"markdown","metadata":{"id":"fgRe1mjg2Vdw","colab_type":"text"},"source":["### 4.5. Building datasets\n","对数据标注：Hovy and Lavid (2010)提出了流程：\n","* Determine what to annotate.\n","*  Optionally, one may design or select a software tool to support the annotation effort. \n","* Formalize the instructions for the annotation task.\n","* Perform a pilot annotation\n","* Annotate the data\n","* Compute and report inter-annotator agreement, and release the data\n","\n","**Measuring inter-annotator agreement**\n","Cohen's Kappa"]}]}