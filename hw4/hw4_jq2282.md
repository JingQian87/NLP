## HW4: Seq2Seq Modeling

#### Jing Qian (jq2282)

### 2.1. Model architecture

![Screen Shot 2019-12-06 at 2.10.06 AM](/Users/mac/Desktop/NLP/hw4/Screen Shot 2019-12-06 at 2.10.06 AM.png)

The seq2seq model is consisted of two main parts: RNN encoder and RNN decoder. For the encoder part, the input is a sequence of input tokens starting with the start-of-sequence (SOS) token and ending with the end-of-sequence (EOS) token. $h_n$ refers to the final hidden state of the encoder. For the decoder part, the decoder input is initialized as the numerical identifier of SOS token and the decoder hidden state is initialized as $h_n$. The RNN decoder outputs a sequence of tokens, starting with SOS token and possibly ending with EOS tokens. If the output reaches the length limit, the final token may not be EOS token.



### 2.2. Error analysis



### 2.3. Beam search analysis



### 2.4. What's wrong with BLEU?



### 2.5. Pyramid scoring for diverse outputs.



### 2.6. How could we deal with unseen restaurant names?

 