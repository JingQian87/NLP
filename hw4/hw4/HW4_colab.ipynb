{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW4_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWaY2R8Nc8CB",
        "colab_type": "text"
      },
      "source": [
        "# Homework 4: Sequence to Sequence Modeling\n",
        "\n",
        "The aim of this homework is to familiarize you with sequence-to-sequence language modeling, specifically using an encoder-decoder model. This is the coding portion; you also have a written portion. The written portion can be found the homework instructions, i.e. the pdf you download from the syllabus website. In this notebook, you are provided with pre-written code for a simple sequence-to-sequence model that already works and learns how to reverse short sequences of numbers.\n",
        "\n",
        "If you run this whole jupyter notebook, it will learn to reverse short sequences of numbers. Although much of this code you will not be modifying, we recommend reading through it to get a sense of how the model and training works.\n",
        "\n",
        "This starter code is based on [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) by Sean Robertson from the PyTorch website. It has been modified by Katy Ilonka Gero for COMS W4705 taught at Columbia University by Professor Kathy McKeown. \n",
        "\n",
        "### Overview\n",
        "\n",
        "Your assignment is to:\n",
        "\n",
        "1. modify this code to run with the E2E restaurant data set (provided)\n",
        "2. train a model on this dataset on a GPU\n",
        "3. implement beam search for evaluation\n",
        "4. implement a BLEU evaluator and report BLEU scores\n",
        "\n",
        "These do not need to be done in this order.\n",
        "\n",
        "You must submit:\n",
        "\n",
        "1. This jupyter notebook, with your solutions to the above assignments. (Note cells that require specific outputs. Do not clear outputs before submitting.)\n",
        "2. A saved model (encoder and decoder) that takes in a meaning representation and generates a restaurant description.\n",
        "\n",
        "Write all your code **in this jupyter notebook**. Cells are provided where you should be implementing your code. See homework instructions for further details on how to submit this homework.\n",
        "\n",
        "### 1. Modify to work with E2E Dataset\n",
        "\n",
        "You will be working with the end-to-end (E2E) challenge dataset. More information can be found on [their website](http://www.macs.hw.ac.uk/InteractionLab/E2E/). In this dataset, the inputs are restaurant meaning representations, which are a series of key-value pairs that encode information about a restaurant. The outputs are fluent sentences that describe the restaurant. Here is an example:\n",
        "\n",
        "*Input: Meaning Representation*\n",
        "\n",
        "```\n",
        "name[The Eagle],\n",
        "eatType[coffee shop],\n",
        "food[French],\n",
        "priceRange[moderate],\n",
        "customerRating[3/5],\n",
        "area[riverside],\n",
        "kidsFriendly[yes],\n",
        "near[Burger King]\n",
        "```\n",
        "\n",
        "*Output: Fluent Sentence*\n",
        "\n",
        "```\n",
        "The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.\n",
        "```\n",
        "\n",
        "You will need to read in and process the training and development data. This data is provided in csv format. Here is an example line from the `trainset.csv` file:\n",
        "\n",
        "```\n",
        "\"name[Browns Cambridge], eatType[coffee shop], food[English], customer rating[5 out of 5], area[riverside], familyFriendly[no], near[Crowne Plaza Hotel]\",\"Browns Cambridge, a 5 out of 5 English coffee shop, is not kid friendly. It is located near Crowne Plaza Hotel and riverside.\"\n",
        "```\n",
        "\n",
        "You will need to tokenize the input and output. The input should be tokenized such that each token is a single entry from the meaning representation. You can decide how to tokenize the output. Here is how the input should be tokenize, and a simple tokenization for the output:\n",
        "\n",
        "*Input:*\n",
        "\n",
        "```\n",
        "['name[Browns Cambridge]', 'eatType[coffee shop]', 'food[English]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Crowne Plaza Hotel]']\n",
        "```\n",
        "\n",
        "*Output:*\n",
        "\n",
        "```\n",
        "['<SOS>', 'Browns', 'Cambridge,', 'a', '5', 'out', 'of', '5', 'English', 'coffee', 'shop,', 'is', 'not', 'kid', 'friendly.', 'It', 'is', 'located', 'near', 'Crowne', 'Plaza', 'Hotel', 'and', 'riverside.', '<EOS>']\n",
        "```\n",
        "\n",
        "Be sure to note the `<SOS>` (start-of-sequence) and `<EOS>` (end-of-sequence) tokens in the output. This is important and necessary! The decoder requires start and end tokens; the start token gives it an initial input to start generating, and the end token lets you know when to stop.\n",
        "\n",
        "Your first goal is to load in this data with [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), a library used to manage text datasets in pytorch. *You do not need to change anything in the model or training or evaluation.* All you need to do is load in the data similar to how the number-reversal data is loaded in.\n",
        "\n",
        "### 2. Train a model on this data\n",
        "\n",
        "To train a model on this data in a reasonable period of time, you will need to run this notebook on the Google Cloud VM with a GPU. [This tutorial](https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52) gives a good explanation of how to use jupyter notebooks from a Google Cloud VM. It can take time to correctly get set up on a GPU, so don't leave this to the last minute. \n",
        "\n",
        "However, we *do* recommend testing your code by loading in a small amount of data (say, 5 examples,) and training on these. This should train quickly even without a GPU and the model should be able to almost memorize the data. This is generally good practice with generative networks -- ensure your model will memorize a small amount of data.\n",
        "\n",
        "With the full e2e dataset on a GPU, it should take around 20 minutes to train a single epoch. You should see decent results after a single epoch. Decent results are sentences with a few mistakes, but are mostly readable. You are encouraged to see what kind of improvements can be found with more training or different parameters.\n",
        "\n",
        "You do not need to modify any code to train the model, nor are you allowed to modify the model. You may modify the `trainIters` function, if you would like to improve how you track progress, or change parameters while training. For example, it can be useful to decrease the teacher-forcing ratio as training progresses.\n",
        "\n",
        "*You must submit a trained model. This model must be a GPU model. It is not reasonable to train this model with a CPU; part of the assignment is training it on a GPU.*\n",
        "\n",
        "*Note that the model is trained using single examples -- that is, it doesn't use batching. Batching is possible with seq2seq models, but for simplicity of reading the code we have not implemented it here.*\n",
        "\n",
        "### 3. Implement a beam search evaluator\n",
        "\n",
        "We provide you with an evaluation function that takes in an input sequence and generates an output sentence given a trained model. This evaluation function performs *greedy decoding* by taking the most likely token at each generation step. You are required to implement a beam search version of this evaluation function, that keeps track of the top *k* most likely sequences at each generation step, and then returns the top *k* best sequences with their associated probabilities.\n",
        "\n",
        "### 4. Implement a BLEU evaluator and report scores\n",
        "\n",
        "While loss and accuracy are good for tracking training progress, they don't tell us much about how well the model generates meaningful sentences. You need to implement a BLEU evaluation function that takes in an input/output pair and returns the BLEU score for how well the model predicts the output.\n",
        "\n",
        "You can find a formal description of how to calculate BLEU in the original paper, [BLEU: A Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf). We reprodue this formal description for you in the homework instructions.\n",
        "\n",
        "When reporting these scores, use the *development dataset* provided. Report scores for greedy decoding and beam search (beam size=3). For beam search, use the top-scoring output sentence as the score for that datapoint.\n",
        "\n",
        "*You must implement your BLEU evaluator from scratch.* There exist python libraries that implement BLEU for you. Do not use these.\n",
        "\n",
        "### Don't forget the written portion!\n",
        "\n",
        "A series of open-ended questions about these tasks are required for the written portion of this homework. Please see the homework instructions for this, as well as instructions about how to submit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_whVTquoc8CG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may modify this cell\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D919AKbc8CN",
        "colab_type": "code",
        "outputId": "6ea1ed4c-61e4-4c93-8407-bd5441b76d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# DO NO MODIFY\n",
        "\n",
        "# this is useful for checking if your code is successfully using the GPU\n",
        "\n",
        "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mydevice"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kAmB5NAc8C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT MODIFY\n",
        "\n",
        "SOS_TOKEN = '<sos>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "\n",
        "MAX_LEN = 50\n",
        "\n",
        "def len_filter(example):\n",
        "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IooFVuqjc8DH",
        "colab_type": "text"
      },
      "source": [
        "### Load dummy number reversal dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP1hNN96dUOw",
        "colab_type": "code",
        "outputId": "6c8e0e3c-bf63-4f70-f2d4-604e68483005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5ANyhUkc8DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # DO NOT MODIFY\n",
        "\n",
        "# train_path = '/content/gdrive/My Drive/NLP/hw4/data/toy_reverse/train/data.txt'\n",
        "# dev_path = '/content/gdrive/My Drive/NLP/hw4/data/toy_reverse/dev/data.txt'\n",
        "\n",
        "# src = torchtext.data.Field(\n",
        "#     batch_first=True, \n",
        "#     include_lengths=True\n",
        "#     )\n",
        "# tgt = torchtext.data.Field(\n",
        "#     batch_first=True, \n",
        "#     preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
        "#     )\n",
        "\n",
        "# data_train = torchtext.data.TabularDataset(\n",
        "#         path=train_path, format='tsv',\n",
        "#         fields=[('src', src), ('tgt', tgt)],\n",
        "#         filter_pred=len_filter\n",
        "#     )\n",
        "# data_dev = torchtext.data.TabularDataset(\n",
        "#         path=dev_path, format='tsv',\n",
        "#         fields=[('src', src), ('tgt', tgt)],\n",
        "#         filter_pred=len_filter\n",
        "#     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAYXBG-c8De",
        "colab_type": "text"
      },
      "source": [
        "### 1. Load the e2e data\n",
        "\n",
        "Load in the E2E data similar to how the dummy number reversal dataset is loaded. That is, use the same `torchtext.data.Field` and `torchtext.data.TabularDataset` classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2282IQ0Hc8Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WRITE YOUR CODE FOR LOADING THE E2E DATA HERE\n",
        "train_path = '/content/gdrive/My Drive/NLP/hw4/data/e2e-dataset/trainset.csv'\n",
        "dev_path = '/content/gdrive/My Drive/NLP/hw4/data/e2e-dataset/devset.csv'\n",
        "\n",
        "src = torchtext.data.Field(\n",
        "    batch_first=True, \n",
        "    tokenize=lambda row: row.split(', '),\n",
        "    include_lengths=True\n",
        "    )\n",
        "tgt = torchtext.data.Field(\n",
        "    batch_first=True, \n",
        "    tokenize=lambda row: row.split(),\n",
        "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
        "    )\n",
        "\n",
        "data_train = torchtext.data.TabularDataset(\n",
        "        path=train_path, format='csv',\n",
        "        skip_header=True,\n",
        "        fields=[('src', src), ('tgt', tgt)],\n",
        "        filter_pred=len_filter\n",
        "    )\n",
        "data_dev = torchtext.data.TabularDataset(\n",
        "        path=dev_path, format='csv',\n",
        "        skip_header=True,\n",
        "        fields=[('src', src), ('tgt', tgt)],\n",
        "        filter_pred=len_filter\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsQu3zGc8Dj",
        "colab_type": "text"
      },
      "source": [
        "Have a look at the vocab and some example data points.\n",
        "\n",
        "*If you have loaded in the E2E dataset correctly, the code in the cell below should work without any modification.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T2As_8qc8Dl",
        "colab_type": "code",
        "outputId": "d4267d8e-a1b5-407c-a5ee-1a3936776953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# You may modify this cell\n",
        "\n",
        "src.build_vocab(data_train, max_size=50000)\n",
        "tgt.build_vocab(data_train, max_size=50000)\n",
        "input_vocab = src.vocab\n",
        "output_vocab = tgt.vocab\n",
        "\n",
        "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
        "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
        "\n",
        "print('\\nnum training examples:', len(data_train.examples))\n",
        "\n",
        "item = random.choice(data_train.examples)\n",
        "print('\\nexample train data:')\n",
        "print('src:\\n', item.src)\n",
        "print('tgt:\\n', item.tgt)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 tokens from input vocab:\n",
            " ['<unk>', '<pad>', 'familyFriendly[yes]', 'area[riverside]', 'eatType[coffee shop]', 'familyFriendly[no]', 'area[city centre]', 'eatType[pub]', 'food[Japanese]', 'food[Italian]', 'food[Fast food]', 'food[French]', 'priceRange[moderate]', 'priceRange[less than £20]', 'customer rating[average]', 'customer rating[low]', 'priceRange[high]', 'customer rating[5 out of 5]', 'priceRange[more than £30]', 'food[Indian]']\n",
            "\n",
            "20 tokens from output vocab:\n",
            " ['<unk>', '<pad>', 'is', '<eos>', '<sos>', 'a', 'The', 'the', 'in', 'near', 'of', 'and', 'food', 'customer', 'located', 'It', 'restaurant', 'has', 'coffee', 'price']\n",
            "\n",
            "num training examples: 42037\n",
            "\n",
            "example train data:\n",
            "src:\n",
            " ['name[The Waterman]', 'food[Fast food]', 'priceRange[less than £20]', 'customer rating[low]', 'area[riverside]', 'familyFriendly[yes]']\n",
            "tgt:\n",
            " ['<sos>', 'The', 'Waterman', 'provides', 'cheap', 'fast', 'food.', 'It', 'is', 'located', 'riverside.', 'It', 'is', 'family-Friendly,', 'but', 'has', 'a', 'low', 'customer', 'rating.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2WV3kTHc8Do",
        "colab_type": "text"
      },
      "source": [
        "### Model definition and training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChWURHmhc8Dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT MODIFY\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, myinput, hidden):\n",
        "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)\n",
        "\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly5qydMKc8Ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT MODIFY\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
        "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    # get an initial hidden state for the encoder\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    # zero the gradients of the optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # get the seq lengths, used for iterating through encoder/decoder\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # create empty tensor to fill with encoder outputs\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
        "\n",
        "    # create a variable for loss\n",
        "    loss = 0\n",
        "    \n",
        "    # pass the inputs through the encoder\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    # create a start-of-sequence tensor for the decoder\n",
        "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
        "\n",
        "    # set the decoder hidden state to the final encoder hidden state\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # decide if we will use teacher forcing\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        \n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "                \n",
        "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
        "        \n",
        "        if use_teacher_forcing:\n",
        "            decoder_input = target_tensor[di]\n",
        "        \n",
        "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UywEYqKNc8Dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may modify this cell\n",
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
        "    print(f'Running {n_iters} epochs...')\n",
        "    print_loss_total = 0\n",
        "    print_loss_epoch = 0\n",
        "\n",
        "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # note batch size of 1, just for simplicity\n",
        "    # DO NOT INCREASE THE BATCH SIZE\n",
        "    batch_iterator = torchtext.data.Iterator(\n",
        "        dataset=data_train, batch_size=1,\n",
        "        sort=False, sort_within_batch=True,\n",
        "        sort_key=lambda x: len(x.src),\n",
        "        device=mydevice, repeat=False)\n",
        "    \n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for e in range(n_iters):\n",
        "        batch_generator = batch_iterator.__iter__()\n",
        "        step = 0\n",
        "        start = time.time()\n",
        "        for batch in batch_generator:\n",
        "            step += 1\n",
        "            \n",
        "            # get the input and target from the batch iterator\n",
        "            input_tensor, input_lengths = getattr(batch, 'src')\n",
        "            target_tensor = getattr(batch, 'tgt')\n",
        "            \n",
        "            # this is because we're not actually using the batches.\n",
        "            # batch size is 1 and this just selects that first one\n",
        "            input_tensor = input_tensor[0]\n",
        "            target_tensor = target_tensor[0]\n",
        "\n",
        "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "            print_loss_total += loss\n",
        "            print_loss_epoch += loss\n",
        "            \n",
        "\n",
        "            if step % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                t = (time.time() - start) / 60\n",
        "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
        "                start = time.time()\n",
        "        \n",
        "        print_loss_avg = print_loss_epoch / step\n",
        "        print_loss_epoch = 0\n",
        "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjn6DR-Fc8Dz",
        "colab_type": "text"
      },
      "source": [
        "### 2. Create and train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN5yLil1c8D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may modify this cell\n",
        "\n",
        "hidden_size = 128\n",
        "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
        "decoder1 = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faW9JDFyc8D6",
        "colab_type": "text"
      },
      "source": [
        "Here are some guidelines for how much training to expect. Note that these *guidelines*; they are not exact.\n",
        "\n",
        "Only 1 epoch is needed for the number reversal dataset. This produces near-perfect results, and should take less than 5 minutes to run on a CPU.\n",
        "\n",
        "To memorize ~5 examples of the e2e dataset, ~100 epochs are needed (with a high teacher forcing ratio). This produces near-perfect results.\n",
        "\n",
        "To train on the full e2e dataset, only 1 epoch is needed to see decent outputs on the training data. More are required to increase fluency and see improvements on the development data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQW-oI8vc8D7",
        "colab_type": "code",
        "outputId": "223d352a-0a23-4a6e-a129-e064fdf0dd91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# You may modify this cell\n",
        "# but be sure that it prints some indication of how training is progressing\n",
        "\n",
        "trainIters(encoder1, decoder1, 1, print_every=1000, teacher_forcing_ratio=0.75)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 1 epochs...\n",
            "step: 1000\t avg loss: 4.31\t time for 1000 steps: 0.38 min\n",
            "step: 2000\t avg loss: 3.60\t time for 1000 steps: 0.39 min\n",
            "step: 3000\t avg loss: 3.42\t time for 1000 steps: 0.38 min\n",
            "step: 4000\t avg loss: 3.24\t time for 1000 steps: 0.38 min\n",
            "step: 5000\t avg loss: 3.10\t time for 1000 steps: 0.40 min\n",
            "step: 6000\t avg loss: 3.03\t time for 1000 steps: 0.39 min\n",
            "step: 7000\t avg loss: 3.07\t time for 1000 steps: 0.41 min\n",
            "step: 8000\t avg loss: 2.94\t time for 1000 steps: 0.39 min\n",
            "step: 9000\t avg loss: 2.84\t time for 1000 steps: 0.39 min\n",
            "step: 10000\t avg loss: 2.82\t time for 1000 steps: 0.40 min\n",
            "step: 11000\t avg loss: 2.72\t time for 1000 steps: 0.40 min\n",
            "step: 12000\t avg loss: 2.66\t time for 1000 steps: 0.39 min\n",
            "step: 13000\t avg loss: 2.60\t time for 1000 steps: 0.40 min\n",
            "step: 14000\t avg loss: 2.65\t time for 1000 steps: 0.39 min\n",
            "step: 15000\t avg loss: 2.64\t time for 1000 steps: 0.39 min\n",
            "step: 16000\t avg loss: 2.62\t time for 1000 steps: 0.39 min\n",
            "step: 17000\t avg loss: 2.57\t time for 1000 steps: 0.40 min\n",
            "step: 18000\t avg loss: 2.57\t time for 1000 steps: 0.39 min\n",
            "step: 19000\t avg loss: 2.49\t time for 1000 steps: 0.39 min\n",
            "step: 20000\t avg loss: 2.48\t time for 1000 steps: 0.40 min\n",
            "step: 21000\t avg loss: 2.51\t time for 1000 steps: 0.39 min\n",
            "step: 22000\t avg loss: 2.51\t time for 1000 steps: 0.39 min\n",
            "step: 23000\t avg loss: 2.52\t time for 1000 steps: 0.39 min\n",
            "step: 24000\t avg loss: 2.48\t time for 1000 steps: 0.39 min\n",
            "step: 25000\t avg loss: 2.51\t time for 1000 steps: 0.40 min\n",
            "step: 26000\t avg loss: 2.45\t time for 1000 steps: 0.40 min\n",
            "step: 27000\t avg loss: 2.39\t time for 1000 steps: 0.39 min\n",
            "step: 28000\t avg loss: 2.43\t time for 1000 steps: 0.39 min\n",
            "step: 29000\t avg loss: 2.40\t time for 1000 steps: 0.40 min\n",
            "step: 30000\t avg loss: 2.40\t time for 1000 steps: 0.40 min\n",
            "step: 31000\t avg loss: 2.42\t time for 1000 steps: 0.39 min\n",
            "step: 32000\t avg loss: 2.42\t time for 1000 steps: 0.39 min\n",
            "step: 33000\t avg loss: 2.43\t time for 1000 steps: 0.39 min\n",
            "step: 34000\t avg loss: 2.43\t time for 1000 steps: 0.40 min\n",
            "step: 35000\t avg loss: 2.39\t time for 1000 steps: 0.40 min\n",
            "step: 36000\t avg loss: 2.31\t time for 1000 steps: 0.40 min\n",
            "step: 37000\t avg loss: 2.39\t time for 1000 steps: 0.39 min\n",
            "step: 38000\t avg loss: 2.41\t time for 1000 steps: 0.39 min\n",
            "step: 39000\t avg loss: 2.32\t time for 1000 steps: 0.39 min\n",
            "step: 40000\t avg loss: 2.31\t time for 1000 steps: 0.40 min\n",
            "step: 41000\t avg loss: 2.28\t time for 1000 steps: 0.40 min\n",
            "step: 42000\t avg loss: 2.36\t time for 1000 steps: 0.39 min\n",
            "End of epoch 0, avg loss 2.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4yXnma6c8D-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WRITE YOUR CODE FOR SAVING YOUR MODEL HERE\n",
        "ENCODER_PATH = '/content/gdrive/My Drive/NLP/hw4/encoder.mdl'\n",
        "DECODER_PATH = '/content/gdrive/My Drive/NLP/hw4/decoder.mdl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaKIznPI4TSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(encoder1.state_dict(), ENCODER_PATH)\n",
        "torch.save(decoder1.state_dict(), DECODER_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT9NKZTQ4QMg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efd62693-0710-43e4-fed0-eff9f5437f74"
      },
      "source": [
        "# We encourage you to confirm that you can load your trained model here also\n",
        "encoder = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
        "decoder = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)\n",
        "encoder.load_state_dict(torch.load(ENCODER_PATH))\n",
        "decoder.load_state_dict(torch.load(DECODER_PATH))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cnz2XE0c8EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT MODIFY\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LEN):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            next_word = output_vocab.itos[topi.item()]\n",
        "            decoded_words.append(next_word)\n",
        "            if next_word == EOS_TOKEN:\n",
        "                break\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGLpltixh3HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(5):\n",
        "#     item = random.choice(data_train.examples)\n",
        "#     seq = item.src\n",
        "#     print(seq)\n",
        "#     words = evaluate(encoder_loaded, decoder_loaded, seq)\n",
        "#     print(' '.join(words))\n",
        "#     print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4_Op_dJc8EI",
        "colab_type": "text"
      },
      "source": [
        "### 3. Implement beam search evaluator\n",
        "\n",
        "Be sure to return all the output sequences (i.e. if the beam size is k, you should return k sequences) and their associated probabilities. You will need the associated probabilities to select the best performing sequence when calculating BLEU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyeArfhZc8EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WRITE YOUR CODE FOR BEAM SEARCH HERE\n",
        "\n",
        "# The output of this cell should be an example input from the dev set, \n",
        "# and three outputs from a beam search evaluator.\n",
        "\n",
        "def evaluate_beam_search(encoder, decoder, sentence, max_length=MAX_LEN, beam_size=3):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Above are the same with greedy evaluate.\n",
        "####################################################################\n",
        "        # Initialize the beams\n",
        "        beams = [] #First element is sequence of tokens, second is corresponding log likelihood.\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.data.topk(beam_size)\n",
        "        for i in range(beam_size):\n",
        "            beams.append([[topi[0][i]], topv[0][i].item()])\n",
        "\n",
        "        # Since above has run 1 decoder, here run max_length-1 times.\n",
        "        for di in range(max_length-1):\n",
        "            candidates = [] # Store k*k candidate seqs.\n",
        "            for ibeam in beams:\n",
        "                decoder_input = ibeam[0][-1] # Take last word in beam seq as input\n",
        "                # If current beam has ended, add it to candidates without expanding\n",
        "                if output_vocab.itos[decoder_input] == EOS_TOKEN:\n",
        "                    candidates.append(ibeam)\n",
        "                    continue \n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "                topv, topi = decoder_output.data.topk(beam_size)\n",
        "                # Add the new words to seqs\n",
        "                for i in range(beam_size):\n",
        "                    candidates.append([ibeam[0]+[topi[0][i]], ibeam[1]+topv[0][i].item()])\n",
        "                    # Since the output of decoder is log likelihood, use summation.\n",
        "            # Choose top k seqs according to length-normalized log-likelihood.\n",
        "            if candidates:\n",
        "                beams = sorted(candidates, key=lambda x:x[1]/len(x[0]), reverse=True)[:beam_size]\n",
        "\n",
        "        # Transform tokens into words:\n",
        "        decoded_words = []\n",
        "        for i in range(beam_size):\n",
        "            tmp_words = []\n",
        "            for j in beams[i][0]:\n",
        "                tmp_words.append(output_vocab.itos[j])\n",
        "            decoded_words.append(tmp_words)\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRaOcsFV1bJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # test beam search1\n",
        "# # paras\n",
        "# sentence = random.choice(data_train.examples).src\n",
        "# max_length=MAX_LEN\n",
        "# beam_size=3\n",
        "\n",
        "# #code\n",
        "# input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
        "# input_length = input_tensor.size()[0]\n",
        "# encoder_hidden = encoder.initHidden()\n",
        "\n",
        "# encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
        "\n",
        "# for ei in range(input_length):\n",
        "#     encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "#     encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "# decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
        "\n",
        "# decoder_hidden = encoder_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sZieezY19ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # test beam search1\n",
        "# beams = [] #first element is sequence, second is corresponding log likelihood.\n",
        "\n",
        "# decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
        "# decoder_hidden = encoder_hidden\n",
        "# decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "# topv, topi = decoder_output.data.topk(beam_size)\n",
        "\n",
        "# for i in range(beam_size):\n",
        "#     beams.append([[topi[0][i]], topv[0][i].item()])\n",
        "# print(beams)\n",
        "# for di in range(max_length-1):\n",
        "#     candidates = []\n",
        "#     for ibeam in beams:\n",
        "#         decoder_input = ibeam[0][-1]\n",
        "#         if output_vocab.itos[decoder_input] == EOS_TOKEN:\n",
        "#             candidates.append(ibeam)\n",
        "#             continue\n",
        "#         decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "#         topv, topi = decoder_output.data.topk(beam_size)\n",
        "#         for i in range(beam_size):\n",
        "#             candidates.append([ibeam[0]+[topi[0][i]], ibeam[1]+topv[0][i].item()])\n",
        "#     if candidates:\n",
        "#         beams = sorted(candidates, key=lambda x:x[1]/len(x[0]), reverse=True)[:beam_size]\n",
        "#     #print(beams)\n",
        "# test = []\n",
        "# for i in beams[0][0]:\n",
        "#     test.append(output_vocab.itos[i])\n",
        "# print(np.shape(test),test)\n",
        "# test = []\n",
        "# for i in beams[1][0]:\n",
        "#     test.append(output_vocab.itos[i])\n",
        "# print(np.shape(test),test)\n",
        "# test = []\n",
        "# for i in beams[2][0]:\n",
        "#     test.append(output_vocab.itos[i])\n",
        "# print(np.shape(test),test)\n",
        "#     # decoder_input = topi.squeeze().detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7I4NKdEc8EM",
        "colab_type": "text"
      },
      "source": [
        "Have a look at some generated sequences! This is the fun part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6toQm7Wc8EQ",
        "colab_type": "code",
        "outputId": "4a191fe9-9036-4b98-d366-a24ff06decae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# You may modify this cell\n",
        "\n",
        "# This selects 5 random datapoints from the training data and shows the generated sequence\n",
        "\n",
        "for i in range(5):\n",
        "    item = random.choice(data_train.examples)\n",
        "    seq = item.src\n",
        "    print(seq)\n",
        "    words = evaluate(encoder, decoder, seq)\n",
        "    print(' '.join(words))\n",
        "    print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['name[Green Man]', 'food[Chinese]', 'priceRange[high]', 'area[riverside]', 'familyFriendly[no]', 'near[All Bar One]']\n",
            "<sos> Green Man is a Chinese restaurant in the price range It is is located in the the is in the the is in the the is the price range in the the is <eos>\n",
            "\n",
            "['name[Giraffe]', 'eatType[coffee shop]', 'priceRange[more than £30]', 'customer rating[high]', 'familyFriendly[yes]', 'near[The Bakers]']\n",
            "<sos> Giraffe is a children friendly coffee shop near The Bakers. It is located near The Bakers. It is located near the riverside. It is near The Bakers. <eos>\n",
            "\n",
            "['name[Fitzbillies]', 'eatType[coffee shop]', 'food[French]', 'priceRange[more than £30]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[yes]']\n",
            "<sos> Fitzbillies is a coffee shop in the It 5 5 star in the price range of 5 <eos>\n",
            "\n",
            "['name[Fitzbillies]', 'eatType[coffee shop]', 'food[French]', 'priceRange[less than £20]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[no]']\n",
            "<sos> Fitzbillies is a coffee shop in the the city the It the city It the is It is is and is not <eos>\n",
            "\n",
            "['name[Blue Spice]', 'priceRange[more than £30]', 'area[riverside]']\n",
            "<sos> Blue Spice is a restaurant in the price range It is <eos>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFKV338Lc8EU",
        "colab_type": "text"
      },
      "source": [
        "### 4. Implement BLEU evaluator\n",
        "\n",
        "Remember that when calculating BLEU using beam search, select the top-scoring sequence output using the model probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oIXCa-ZGfo_r",
        "colab": {}
      },
      "source": [
        "# The output of this cell should be the average BLEU score on the dev set\n",
        "# for greedy decoding AND for beam search decoding (beam size = 3)\n",
        "import string\n",
        "# Preprocess the seq from data_dev\n",
        "def preprocessing(seq):\n",
        "    if EOS_TOKEN in seq:\n",
        "        seq.remove(EOS_TOKEN)\n",
        "    if SOS_TOKEN in seq:\n",
        "        seq.remove(SOS_TOKEN)\n",
        "    seq = [i.translate(str.maketrans('','',string.punctuation)).lower() for i in seq]\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzRuVq_Kc8EV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed76769b-6a23-4a8d-d386-21dfa5261938"
      },
      "source": [
        "# WRITE YOUR CODE FOR THE BLEU EVALUATION HERE\n",
        "# Get the src-tgt dictionary  in dev data.\n",
        "dev_dict = {}\n",
        "for i in data_dev.examples:\n",
        "    key = tuple(i.src)\n",
        "    if key not in dev_dict:\n",
        "        dev_dict[key] = [preprocessing(i.tgt)]\n",
        "    else:\n",
        "        dev_dict[key] = dev_dict[key]+[preprocessing(i.tgt)]\n",
        "print(len(dev_dict))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXHFnDRRRy7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate pn: modified precision scores for a set of ngrams.\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "def pn(src, tgt_list, n):\n",
        "    counts = Counter(ngrams(src,n))\n",
        "    max_counts = {}\n",
        "    for i in tgt_list:\n",
        "        reference_counts = Counter(ngrams(i,n))\n",
        "        for ngram in counts:\n",
        "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
        "\n",
        "    clipped_counts = {ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()}\n",
        "    return sum(clipped_counts.values())/max(1, sum(counts.values())) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahVGJWaWRy12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the Brevity Penalty\n",
        "import math\n",
        "def brevity_penalty(tgt_list, src_len):\n",
        "    ref_lens = [len(i) for i in tgt_list]\n",
        "    closest_ref_len = min(ref_lens, key = lambda ref_len: (abs(ref_len-src_len), ref_len))\n",
        "    if closest_ref_len <= src_len:\n",
        "        return 1\n",
        "    else:\n",
        "        return math.exp(1-closest_ref_len/src_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynk529psd0C7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate BLEU4\n",
        "def BLEU4(src, tgt_list):\n",
        "    p1 = pn(src, tgt_list, 1)\n",
        "    p2 = pn(src, tgt_list, 2)\n",
        "    p3 = pn(src, tgt_list, 3)\n",
        "    p4 = pn(src, tgt_list, 4)\n",
        "    if p1 == 0:\n",
        "        avg = 0\n",
        "    elif p2 == 0:\n",
        "        avg = p1\n",
        "    elif p3 == 0:\n",
        "        avg = (p1*p2)**0.5\n",
        "    elif p4 == 0:\n",
        "        avg = (p1*p1*p3)**(1/3)\n",
        "    else:\n",
        "        avg = (p1*p1*p3*p4)**(1/4)\n",
        "    return avg*brevity_penalty(tgt_list, len(src))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWVXuokf4kCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# sentence_bleu(value, candidate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mPsG6lBhoxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bleu_greedy = []\n",
        "for key, value in dev_dict.items():\n",
        "    candidate = preprocessing(evaluate(encoder, decoder, key))\n",
        "    bleu_greedy.append(BLEU4(candidate, value))\n",
        "avg_bleu_greedy = sum(bleu_greedy)/len(bleu_greedy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBGYKInYnOkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3341373-2af1-4bf7-d57f-4643757e2149"
      },
      "source": [
        "avg_bleu_greedy"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4393657025745546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbipSc-B46Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bleu_beam = []\n",
        "for key, value in dev_dict.items():\n",
        "    candidates = evaluate_beam_search(encoder, decoder, key)\n",
        "    max_bleu = 0\n",
        "    for ican in candidates:\n",
        "        max_bleu = max(max_bleu, BLEU4(preprocessing(ican), value))\n",
        "    bleu_beam.append(max_bleu)\n",
        "avg_bleu_beam = sum(bleu_beam)/len(bleu_beam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQJF6xfYn6i4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86627cf2-9a4f-4cd2-ab58-35c6f645d3d6"
      },
      "source": [
        "avg_bleu_beam"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45755794719648657"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx1PBvNNHvdU",
        "colab_type": "text"
      },
      "source": [
        "## error analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8DpjpOWaoZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a1097e15-b1ac-4ef9-8281-5c92a1d1e6e7"
      },
      "source": [
        "key"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('name[Wildwood]',\n",
              " 'eatType[coffee shop]',\n",
              " 'food[English]',\n",
              " 'priceRange[moderate]',\n",
              " 'customer rating[3 out of 5]',\n",
              " 'near[Ranch]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmcXEZYVKUzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "66b06277-0b6f-4ab1-b21e-5274adbafd3e"
      },
      "source": [
        "test_name = ('name[Jings Malatang]','eatType[coffee shop]', 'priceRange[high]', 'area[city centre]', 'near[The Sorrento]')\n",
        "candidate = evaluate(encoder, decoder, test_name)\n",
        "print(\" \".join(candidate))\n",
        "candidates = evaluate_beam_search(encoder, decoder, test_name)\n",
        "for ican in candidates:\n",
        "    print(\" \".join(ican))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> The Mill is a coffee shop located near The near The coffee shop in the in the <eos>\n",
            "<sos> The Sorrento is a coffee shop that serves Indian food. <eos>\n",
            "<sos> Near The Mill is a coffee shop that <eos>\n",
            "<sos> The Sorrento is a coffee shop that serves Indian food <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCLCAm3rnDzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4316d305-1ff7-49fb-ed44-267a10acb43e"
      },
      "source": [
        "i = 0\n",
        "for key, value in dev_dict.items():\n",
        "    i+=1\n",
        "    if i%20!=0:continue\n",
        "    print(i,'\\n')\n",
        "    print(key)\n",
        "    candidate = evaluate(encoder, decoder, key)\n",
        "    print(\"GREEDY PREDICT:\")\n",
        "    print(\" \".join(candidate))\n",
        "    print(BLEU4(candidate, value))\n",
        "    print(\"\\n\")\n",
        "    print(\"BEAM PREDICT:\")\n",
        "    candidates = evaluate_beam_search(encoder, decoder, key)\n",
        "    for ican in candidates:\n",
        "        print(\" \".join(ican))\n",
        "        print( BLEU4(preprocessing(ican), value))\n",
        "    # print(\"\\n REFERENCE:\")\n",
        "    # for ival in value:\n",
        "    #     print(\" \".join(ival))\n",
        "    #print(BLEU4(candidate, value),\"\\n\")\n",
        "   "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 \n",
            "\n",
            "('name[Aromi]', 'eatType[coffee shop]', 'food[Chinese]', 'customer rating[low]', 'area[city centre]', 'familyFriendly[no]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Aromi is a coffee shop in the city centre. It city centre. It is not family-friendly. It is not family-friendly. <eos>\n",
            "0.30051459922164164\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Aromi is a coffee shop serving Chinese food is not family friendly. <eos>\n",
            "0.5890769257099558\n",
            "<sos> Aromi is a coffee shop serving Chinese food. <eos>\n",
            "0.33483239880625626\n",
            "<sos> Aromi in the coffee shop serving Chinese food. <eos>\n",
            "0.3277793447161845\n",
            "40 \n",
            "\n",
            "('name[Bibimbap House]', 'area[riverside]', 'near[Café Sicilia]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Near the riverside near the Café is called The Dumpling Tree is a family friendly family friendly coffee shop <eos>\n",
            "0.2760262237369417\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Near the riverside is a family friendly restaurant. <eos>\n",
            "0.44153854132913556\n",
            "<sos> Near the riverside is a family friendly restaurant called Clowns is located near Clare Hall. <eos>\n",
            "0.3025424941047335\n",
            "<sos> Near the riverside is a family friendly restaurant called Clowns is located near the river. <eos>\n",
            "0.3245572960558128\n",
            "60 \n",
            "\n",
            "('name[Browns Cambridge]', 'eatType[coffee shop]', 'food[English]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Crowne Plaza Hotel]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Browns Cambridge is a coffee shop located near the city Crowne the city It near the Crowne Plaza the in the city 5 the customer rating 5 of 5 out of 5 and is not family-friendly. 5 out of 5 by customers. <eos>\n",
            "0.20451245109882074\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> There is a coffee shop in the city centre of the city centre. <eos>\n",
            "0.15567701734935893\n",
            "<sos> There is a coffee shop in the city centre of the city centre and has a 5 out of 5 by customers. <eos>\n",
            "0.39990753645233373\n",
            "<sos> There is a coffee shop in the city centre of the city centre and has a 5 out of 5 and is not family-friendly. <eos>\n",
            "0.36275382698457337\n",
            "80 \n",
            "\n",
            "('name[Clowns]', 'eatType[coffee shop]', 'food[English]', 'customer rating[1 out of 5]', 'area[riverside]', 'near[Clare Hall]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Clowns is a coffee shop near Clare Hall in the the the riverside area the in the <eos>\n",
            "0.2850334739405197\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Clowns is a coffee shop that serves English food is rated 1 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5. <eos>\n",
            "0.28968467762366895\n",
            "<sos> Clowns is a coffee shop that serves English food is rated 1 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5. <eos>\n",
            "0.31710075939710874\n",
            "<sos> Clowns is a coffee shop that serves English food is rated 1 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5. <eos>\n",
            "0.35025465258988775\n",
            "100 \n",
            "\n",
            "('name[Cocum]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'customer rating[low]', 'familyFriendly[yes]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Cocum is a family friendly coffee shop that provides low priced Chinese food. <eos>\n",
            "0.3448371180801331\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Cocum is a family coffee shop serving Chinese food. <eos>\n",
            "0.20234119849727505\n",
            "<sos> Cocum is a family coffee shop with low customer rating is low. <eos>\n",
            "0.3289951859693163\n",
            "<sos> Cocum is a family coffee shop with low customer rating is located <eos>\n",
            "0.3149889093683611\n",
            "120 \n",
            "\n",
            "('name[Cocum]', 'eatType[coffee shop]', 'food[English]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'familyFriendly[yes]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Cocum is a coffee shop that shop with a moderate price It is and is friendly. It is a 1 out of 5 <eos>\n",
            "0.42551621435268044\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Cocum is a coffee shop for families. <eos>\n",
            "0.3532222238424144\n",
            "<sos> Cocum is a coffee shop for a family friendly coffee shop serving sushi. <eos>\n",
            "0.48159298533238276\n",
            "<sos> Cocum is a coffee shop for a family friendly coffee shop that serves sushi. <eos>\n",
            "0.5386503770879573\n",
            "140 \n",
            "\n",
            "('name[Cotto]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[more than £30]', 'customer rating[high]', 'area[riverside]', 'near[The Portland Arms]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Cotto is a coffee shop located near The Portland Arms in the Portland Arms a the price range It is It is of <eos>\n",
            "0.192252757218943\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Cotto is Portland Arms is a coffee shop for its price range and is 1 out of 5 by its customers. <eos>\n",
            "0.29448773425712943\n",
            "<sos> Cotto is Portland Arms is a coffee shop for its price range and is low. <eos>\n",
            "0.2489918269150431\n",
            "<sos> Cotto is Portland Arms is a coffee shop for its price range and is 1 out of 5. <eos>\n",
            "0.2724492760281117\n",
            "160 \n",
            "\n",
            "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[£20-25]', 'customer rating[high]', 'area[riverside]', 'familyFriendly[no]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Fitzbillies is a coffee shop in the city It the in the the city It a It is It is range It is and is not not a is friendly. <eos>\n",
            "0.22667547112013808\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Fitzbillies is a coffee shop in the customer rating is located in the city centre. <eos>\n",
            "0.36356595284468696\n",
            "<sos> Fitzbillies is a coffee shop with a customer rating is located in the city centre. <eos>\n",
            "0.3094731899981711\n",
            "<sos> Fitzbillies is a coffee shop in the customer rating is located in the city centre and is not child friendly. <eos>\n",
            "0.4538738982375388\n",
            "180 \n",
            "\n",
            "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'customer rating[low]', 'area[riverside]', 'familyFriendly[no]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Fitzbillies is a low priced family friendly coffee shop located in the city centre. <eos>\n",
            "0.2661086980925717\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Fitzbillies is a coffee shop located in the city centre. <eos>\n",
            "0.21368407790943453\n",
            "<sos> Fitzbillies is a coffee shop located near the city centre. <eos>\n",
            "0.24662620804789936\n",
            "<sos> Fitzbillies is a coffee shop located in the city centre and is not family-friendly. <eos>\n",
            "0.32965716557622554\n",
            "200 \n",
            "\n",
            "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[English]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Fitzbillies is a cheap coffee shop in the city the the city 5 the the the city 5 centre. It is 5 and is not family-friendly. 5 out of 5 <eos>\n",
            "0.21201340644934405\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Fitzbillies is a coffee shop with a 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of the city centre. <eos>\n",
            "0.2534508823139693\n",
            "<sos> Fitzbillies is a coffee shop with a 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of <eos>\n",
            "0.2982447283680503\n",
            "<sos> Fitzbillies is a coffee shop with a 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of 5 out of the city centre not family-friendly. <eos>\n",
            "0.256558065786937\n",
            "220 \n",
            "\n",
            "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[English]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[no]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Fitzbillies is a moderately priced coffee shop in the city centre. It is the of the is and is not It is is of <eos>\n",
            "0.29793079067805034\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Fitzbillies is a coffee shop for families. <eos>\n",
            "0.0696841195844544\n",
            "<sos> Fitzbillies is a coffee shop for families. It is located in the city centre. <eos>\n",
            "0.2705966071326179\n",
            "<sos> Fitzbillies is a coffee shop for families. It is located in the city centre and is not family friendly. <eos>\n",
            "0.365102084142423\n",
            "240 \n",
            "\n",
            "('name[Taste of Cambridge]', 'eatType[coffee shop]', 'food[English]', 'area[city centre]', 'familyFriendly[yes]', 'near[Crowne Plaza Hotel]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Taste of Cambridge is a coffee shop located near the city in the the city It near Crowne the city near Crowne the of Crowne the city It Crowne the of is and is is is <eos>\n",
            "0.1305226767827416\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Taste of the coffee shop near Crowne Plaza Hotel near the Crowne Plaza Hotel. <eos>\n",
            "0.34874153371596506\n",
            "<sos> Taste of the coffee shop near Crowne Plaza Hotel near Crowne Plaza Hotel. <eos>\n",
            "0.2079804756016975\n",
            "<sos> Taste of the coffee shop near Crowne Plaza Hotel near the Crowne Plaza Hotel and is not family friendly. <eos>\n",
            "0.36422261492623736\n",
            "260 \n",
            "\n",
            "('name[The Cricketers]', 'eatType[coffee shop]', 'food[Chinese]', 'customer rating[5 out of 5]', 'familyFriendly[no]', 'near[The Portland Arms]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Cricketers is a coffee shop providing Japanese food Portland Arms and is rated 5 out of 5 and is not family-friendly. <eos>\n",
            "0.40069591431357804\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Cricketers is a coffee shop serving Japanese food and is not family friendly. <eos>\n",
            "0.3043353092939372\n",
            "<sos> The Cricketers is a coffee shop serving Japanese food and is not family-friendly. <eos>\n",
            "0.2215766746841697\n",
            "<sos> The Cricketers is a coffee shop serving Japanese food. It is not family friendly. <eos>\n",
            "0.3611676225984304\n",
            "280 \n",
            "\n",
            "('name[The Eagle]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Café Brazil]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Eagle is a restaurant located in the city the It city 5 the near city 5 the near city 5 The customer rating out of 5 of 5 and is not <eos>\n",
            "0.20613214978037914\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Eagle near the city centre, is a 5 out of 5 out of 5 out of 5 out of 5. <eos>\n",
            "0.28304316079961067\n",
            "<sos> The Eagle near the city centre, is a 5 out of 5 out of 5 out of 5 out of 5 by its not of £20-25. <eos>\n",
            "0.24534246787477418\n",
            "<sos> The Eagle near the city centre, is a 5 out of 5 out of 5 out of 5 out of 5 by its price range is not family-friendly. <eos>\n",
            "0.25777350376681235\n",
            "300 \n",
            "\n",
            "('name[The Eagle]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[cheap]', 'customer rating[average]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Eagle is a cheap family friendly coffee shop located near Burger King. It provides Indian food. It is located in the city centre. <eos>\n",
            "0.24534246787477418\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Eagle is located near Burger King the city centre. <eos>\n",
            "0.16090468879834804\n",
            "<sos> The Eagle is located near Burger King Burger King. <eos>\n",
            "0.0941460116785277\n",
            "<sos> The Eagle is located near Burger King Burger King. It is a family friendly coffee shop that serves French food. <eos>\n",
            "0.3195220230551605\n",
            "320 \n",
            "\n",
            "('name[The Eagle]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[moderate]', 'customer rating[3 out of 5]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Eagle is a coffee shop providing Indian food in the moderate price range. It It is located in the city It city centre. It is and is kid friendly. It has a customer rating of 3 out of 5. <eos>\n",
            "0.4572430082372548\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Eagle near Burger coffee shop with moderate price range of 5 out of 3 out of 5 out of 5. <eos>\n",
            "0.21531572369287996\n",
            "<sos> The Eagle near Burger coffee shop with moderate price range of 5 out of 5 out of 5. <eos>\n",
            "0.21710877159989508\n",
            "<sos> The Eagle near Burger coffee shop with moderate price range of 5 out of 5 out of 5 by customers. <eos>\n",
            "0.22988146133756027\n",
            "340 \n",
            "\n",
            "('name[The Eagle]', 'eatType[coffee shop]', 'food[English]', 'priceRange[high]', 'customer rating[1 out of 5]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Eagle is a coffee shop in the city It It is It is is It is is is of is and is Burger is is Burger King. is <eos>\n",
            "0.1355816640980153\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> There is a coffee shop in the city centre. It is not child friendly. <eos>\n",
            "0.14891153781412952\n",
            "<sos> There is a coffee shop in the city centre. It is child friendly. <eos>\n",
            "0.13566147430935332\n",
            "<sos> There is a coffee shop in the city centre. It is rated 1 out of 5 and is not child friendly. <eos>\n",
            "0.29471904119085457\n",
            "360 \n",
            "\n",
            "('name[The Eagle]', 'eatType[coffee shop]', 'food[English]', 'priceRange[more than £30]', 'customer rating[high]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Eagle is a coffee shop in the city It city It It is is is is It is is is is and is is is Burger is Burger King. is is is is and is friendly. <eos>\n",
            "0.12906787032377806\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Eagle is The Eagle is a coffee shop serving Japanese food. It is rated 5 out of the price range of more than £30. <eos>\n",
            "0.4298810542954462\n",
            "<sos> The Eagle is The Eagle is a coffee shop serving Japanese food. It is rated 5 out of 5. <eos>\n",
            "0.30941516886679665\n",
            "<sos> The Eagle is The Eagle is a coffee shop serving Japanese food. It is rated 5 out of the price range of £20-25. <eos>\n",
            "0.2724716333248286\n",
            "380 \n",
            "\n",
            "('name[The Golden Palace]', 'eatType[coffee shop]', 'food[English]', 'priceRange[£20-25]', 'customer rating[high]', 'area[city centre]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Golden Palace is a coffee shop in the city city centre. It a city centre. It is It is is and is <eos>\n",
            "0.24819194241699596\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Golden Palace coffee shop that serves food and has an average customer rating. <eos>\n",
            "0.1967230184280761\n",
            "<sos> The Golden Palace coffee shop that serves food and has an average rating. <eos>\n",
            "0.18091659729295959\n",
            "<sos> The Golden Palace coffee shop that serves food and has an average customer rating is a high price. <eos>\n",
            "0.24613782311589247\n",
            "400 \n",
            "\n",
            "('name[The Mill]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[high]', 'area[city centre]', 'near[The Sorrento]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Mill is a coffee shop located near The located in the The Sorrento. <eos>\n",
            "0.23801485533558359\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Mill is a coffee shop located in the city centre. <eos>\n",
            "0.48394497834347944\n",
            "<sos> The Mill is a coffee shop located in the city centre near The Sorrento. <eos>\n",
            "0.7121408698238745\n",
            "<sos> The Mill is a coffee shop that serves Chinese food. <eos>\n",
            "0.4965853037914095\n",
            "420 \n",
            "\n",
            "('name[The Phoenix]', 'customer rating[average]', 'area[city centre]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Phoenix is an average rated restaurant in the city centre city centre. <eos>\n",
            "0.22986864711558164\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Phoenix is an average customer rating is rating. <eos>\n",
            "0.4079818227493486\n",
            "<sos> The Phoenix is an average and an average and is food at an <eos>\n",
            "0.26853920796458286\n",
            "<sos> The Phoenix is an average and an average and is food at The Phoenix <eos>\n",
            "0.2561917968946907\n",
            "440 \n",
            "\n",
            "('name[The Punter]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[£20-25]', 'customer rating[high]', 'familyFriendly[no]', 'near[Café Sicilia]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Punter is a coffee shop providing Chinese food It is near Café is and is It is is is It is and is not friendly. It is and is not friendly. <eos>\n",
            "0.1512413119573581\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Punter is a coffee shop that is and Punter is not child friendly. <eos>\n",
            "0.24400487378314384\n",
            "<sos> The Punter is a coffee shop that is and Punter is not children friendly. <eos>\n",
            "0.24400487378314384\n",
            "<sos> The Punter is a coffee shop that is and rated and is not friendly. <eos>\n",
            "0.2559144706204023\n",
            "460 \n",
            "\n",
            "('name[The Punter]', 'eatType[coffee shop]', 'food[English]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'familyFriendly[no]', 'near[Café Sicilia]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Punter is a cheap coffee shop near Café Sicilia. It is is 5 out of 5 and is not family-friendly. <eos>\n",
            "0.3391646586813851\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Punter is a coffee shop that is not family friendly. <eos>\n",
            "0.45430572036914546\n",
            "<sos> The Punter is a coffee shop that is not family-friendly. <eos>\n",
            "0.5056019242729417\n",
            "<sos> The Punter is a coffee shop that is not family-friendly. They are not family-friendly. <eos>\n",
            "0.4655180970154643\n",
            "480 \n",
            "\n",
            "('name[The Wrestlers]', 'customer rating[5 out of 5]', 'familyFriendly[yes]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Wrestlers is a family friendly five star family friendly restaurant. <eos>\n",
            "0.22773126241644526\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> There is a family friendly restaurant with a 5 out of 5 by customers. <eos>\n",
            "0.9206877118793398\n",
            "<sos> There is a family friendly restaurant with a 5 out of 5 <eos>\n",
            "0.9054786533927361\n",
            "<sos> There is a family friendly restaurant with a 5 out of 5. <eos>\n",
            "0.9054786533927361\n",
            "500 \n",
            "\n",
            "('name[The Wrestlers]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'area[riverside]', 'familyFriendly[no]', 'near[Raja Indian Cuisine]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Wrestlers is a family friendly coffee shop located near Raja Indian Cuisine. It is in the city centre. <eos>\n",
            "0.1908345380707581\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Near Raja Indian coffee shop that is not family friendly. <eos>\n",
            "0.13746786361638214\n",
            "<sos> Near Raja Indian coffee shop that is not family friendly. It is located in the city centre of the <eos>\n",
            "0.38483265444233633\n",
            "<sos> Near Raja Indian coffee shop that is not family friendly. It is located in the riverside and is The Punter <eos>\n",
            "0.4721184338144005\n",
            "520 \n",
            "\n",
            "('name[The Wrestlers]', 'eatType[coffee shop]', 'food[English]', 'priceRange[moderate]', 'area[riverside]', 'familyFriendly[yes]', 'near[Raja Indian Cuisine]')\n",
            "GREEDY PREDICT:\n",
            "<sos> The Wrestlers is a family friendly coffee shop that serves Japanese in the in the is It is of Raja <eos>\n",
            "0.2928106935986605\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> The Wrestlers is a coffee shop with a Raja Indian Cuisine. <eos>\n",
            "0.4288279737668858\n",
            "<sos> The Wrestlers is a coffee shop with a Raja and is located in the riverside area and is rated 1 out of 5. <eos>\n",
            "0.46191836696974076\n",
            "<sos> The Wrestlers is a coffee shop near Raja of Raja Indian Cuisine. <eos>\n",
            "0.48850996830091914\n",
            "540 \n",
            "\n",
            "('name[Wildwood]', 'eatType[coffee shop]', 'food[English]', 'priceRange[£20-25]', 'customer rating[high]', 'near[Ranch]')\n",
            "GREEDY PREDICT:\n",
            "<sos> Wildwood is a coffee shop with a customer rating of 5 out of 5 near Ranch and is a is a the price <eos>\n",
            "0.19086696487666654\n",
            "\n",
            "\n",
            "BEAM PREDICT:\n",
            "<sos> Wildwood is a coffee shop with a high customer rating of 5 out of five stars. <eos>\n",
            "0.32297890031631327\n",
            "<sos> Wildwood is a coffee shop with a high customer rating of five star rating. <eos>\n",
            "0.3594706492555191\n",
            "<sos> Wildwood is a coffee shop with a high customer rating of 5 out of five stars. of the city centre and is near the Ranch. <eos>\n",
            "0.23996096855357554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB3bCXvlIICO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73f31fd7-b26c-47a3-b8fe-6087bca5929c"
      },
      "source": [
        "import numpy as np\n",
        "np.shape(data_train.examples)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42037,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbtxYVl0LQz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "db7f2713-9e9c-472a-8f33-f1b81f61ca2c"
      },
      "source": [
        "for i in range(1000):\n",
        "    if data_train.examples[i].src[0] == 'name[The Eagle]':\n",
        "        print(data_train.examples[i].src)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Japanese]', 'priceRange[less than £20]', 'customer rating[low]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'food[Chinese]', 'customer rating[1 out of 5]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[French]', 'priceRange[more than £30]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[French]', 'priceRange[more than £30]', 'customer rating[low]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[high]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'food[English]', 'customer rating[5 out of 5]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[French]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[city centre]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[riverside]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[French]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Indian]', 'priceRange[moderate]', 'customer rating[3 out of 5]', 'area[city centre]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[city centre]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'food[Fast food]', 'customer rating[low]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'food[French]', 'customer rating[3 out of 5]']\n",
            "['name[The Eagle]', 'priceRange[more than £30]', 'customer rating[high]', 'area[riverside]']\n",
            "['name[The Eagle]', 'food[English]', 'customer rating[5 out of 5]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Fast food]', 'priceRange[high]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Fast food]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[French]', 'priceRange[less than £20]', 'customer rating[low]', 'area[city centre]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Japanese]', 'priceRange[high]', 'customer rating[3 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'priceRange[£20-25]', 'customer rating[3 out of 5]', 'area[riverside]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Indian]', 'priceRange[less than £20]', 'customer rating[low]', 'area[city centre]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[more than £30]', 'customer rating[high]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'food[Italian]', 'customer rating[average]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Fast food]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[cheap]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[no]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Indian]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Italian]', 'priceRange[cheap]', 'customer rating[average]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Japanese]', 'priceRange[more than £30]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Japanese]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Fast food]', 'priceRange[high]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'priceRange[£20-25]', 'customer rating[3 out of 5]', 'area[riverside]']\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Indian]', 'priceRange[moderate]', 'customer rating[3 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "['name[The Eagle]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[city centre]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqXeKwAlLZav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}