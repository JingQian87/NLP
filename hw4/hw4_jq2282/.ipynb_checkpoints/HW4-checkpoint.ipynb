{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWaY2R8Nc8CB"
   },
   "source": [
    "# Homework 4: Sequence to Sequence Modeling\n",
    "\n",
    "The aim of this homework is to familiarize you with sequence-to-sequence language modeling, specifically using an encoder-decoder model. This is the coding portion; you also have a written portion. The written portion can be found the homework instructions, i.e. the pdf you download from the syllabus website. In this notebook, you are provided with pre-written code for a simple sequence-to-sequence model that already works and learns how to reverse short sequences of numbers.\n",
    "\n",
    "If you run this whole jupyter notebook, it will learn to reverse short sequences of numbers. Although much of this code you will not be modifying, we recommend reading through it to get a sense of how the model and training works.\n",
    "\n",
    "This starter code is based on [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) by Sean Robertson from the PyTorch website. It has been modified by Katy Ilonka Gero for COMS W4705 taught at Columbia University by Professor Kathy McKeown. \n",
    "\n",
    "### Overview\n",
    "\n",
    "Your assignment is to:\n",
    "\n",
    "1. modify this code to run with the E2E restaurant data set (provided)\n",
    "2. train a model on this dataset on a GPU\n",
    "3. implement beam search for evaluation\n",
    "4. implement a BLEU evaluator and report BLEU scores\n",
    "\n",
    "These do not need to be done in this order.\n",
    "\n",
    "You must submit:\n",
    "\n",
    "1. This jupyter notebook, with your solutions to the above assignments. (Note cells that require specific outputs. Do not clear outputs before submitting.)\n",
    "2. A saved model (encoder and decoder) that takes in a meaning representation and generates a restaurant description.\n",
    "\n",
    "Write all your code **in this jupyter notebook**. Cells are provided where you should be implementing your code. See homework instructions for further details on how to submit this homework.\n",
    "\n",
    "### 1. Modify to work with E2E Dataset\n",
    "\n",
    "You will be working with the end-to-end (E2E) challenge dataset. More information can be found on [their website](http://www.macs.hw.ac.uk/InteractionLab/E2E/). In this dataset, the inputs are restaurant meaning representations, which are a series of key-value pairs that encode information about a restaurant. The outputs are fluent sentences that describe the restaurant. Here is an example:\n",
    "\n",
    "*Input: Meaning Representation*\n",
    "\n",
    "```\n",
    "name[The Eagle],\n",
    "eatType[coffee shop],\n",
    "food[French],\n",
    "priceRange[moderate],\n",
    "customerRating[3/5],\n",
    "area[riverside],\n",
    "kidsFriendly[yes],\n",
    "near[Burger King]\n",
    "```\n",
    "\n",
    "*Output: Fluent Sentence*\n",
    "\n",
    "```\n",
    "The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.\n",
    "```\n",
    "\n",
    "You will need to read in and process the training and development data. This data is provided in csv format. Here is an example line from the `trainset.csv` file:\n",
    "\n",
    "```\n",
    "\"name[Browns Cambridge], eatType[coffee shop], food[English], customer rating[5 out of 5], area[riverside], familyFriendly[no], near[Crowne Plaza Hotel]\",\"Browns Cambridge, a 5 out of 5 English coffee shop, is not kid friendly. It is located near Crowne Plaza Hotel and riverside.\"\n",
    "```\n",
    "\n",
    "You will need to tokenize the input and output. The input should be tokenized such that each token is a single entry from the meaning representation. You can decide how to tokenize the output. Here is how the input should be tokenize, and a simple tokenization for the output:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "```\n",
    "['name[Browns Cambridge]', 'eatType[coffee shop]', 'food[English]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Crowne Plaza Hotel]']\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "\n",
    "```\n",
    "['<SOS>', 'Browns', 'Cambridge,', 'a', '5', 'out', 'of', '5', 'English', 'coffee', 'shop,', 'is', 'not', 'kid', 'friendly.', 'It', 'is', 'located', 'near', 'Crowne', 'Plaza', 'Hotel', 'and', 'riverside.', '<EOS>']\n",
    "```\n",
    "\n",
    "Be sure to note the `<SOS>` (start-of-sequence) and `<EOS>` (end-of-sequence) tokens in the output. This is important and necessary! The decoder requires start and end tokens; the start token gives it an initial input to start generating, and the end token lets you know when to stop.\n",
    "\n",
    "Your first goal is to load in this data with [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), a library used to manage text datasets in pytorch. *You do not need to change anything in the model or training or evaluation.* All you need to do is load in the data similar to how the number-reversal data is loaded in.\n",
    "\n",
    "### 2. Train a model on this data\n",
    "\n",
    "To train a model on this data in a reasonable period of time, you will need to run this notebook on the Google Cloud VM with a GPU. [This tutorial](https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52) gives a good explanation of how to use jupyter notebooks from a Google Cloud VM. It can take time to correctly get set up on a GPU, so don't leave this to the last minute. \n",
    "\n",
    "However, we *do* recommend testing your code by loading in a small amount of data (say, 5 examples,) and training on these. This should train quickly even without a GPU and the model should be able to almost memorize the data. This is generally good practice with generative networks -- ensure your model will memorize a small amount of data.\n",
    "\n",
    "With the full e2e dataset on a GPU, it should take around 20 minutes to train a single epoch. You should see decent results after a single epoch. Decent results are sentences with a few mistakes, but are mostly readable. You are encouraged to see what kind of improvements can be found with more training or different parameters.\n",
    "\n",
    "You do not need to modify any code to train the model, nor are you allowed to modify the model. You may modify the `trainIters` function, if you would like to improve how you track progress, or change parameters while training. For example, it can be useful to decrease the teacher-forcing ratio as training progresses.\n",
    "\n",
    "*You must submit a trained model. This model must be a GPU model. It is not reasonable to train this model with a CPU; part of the assignment is training it on a GPU.*\n",
    "\n",
    "*Note that the model is trained using single examples -- that is, it doesn't use batching. Batching is possible with seq2seq models, but for simplicity of reading the code we have not implemented it here.*\n",
    "\n",
    "### 3. Implement a beam search evaluator\n",
    "\n",
    "We provide you with an evaluation function that takes in an input sequence and generates an output sentence given a trained model. This evaluation function performs *greedy decoding* by taking the most likely token at each generation step. You are required to implement a beam search version of this evaluation function, that keeps track of the top *k* most likely sequences at each generation step, and then returns the top *k* best sequences with their associated probabilities.\n",
    "\n",
    "### 4. Implement a BLEU evaluator and report scores\n",
    "\n",
    "While loss and accuracy are good for tracking training progress, they don't tell us much about how well the model generates meaningful sentences. You need to implement a BLEU evaluation function that takes in an input/output pair and returns the BLEU score for how well the model predicts the output.\n",
    "\n",
    "You can find a formal description of how to calculate BLEU in the original paper, [BLEU: A Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf). We reprodue this formal description for you in the homework instructions.\n",
    "\n",
    "When reporting these scores, use the *development dataset* provided. Report scores for greedy decoding and beam search (beam size=3). For beam search, use the top-scoring output sentence as the score for that datapoint.\n",
    "\n",
    "*You must implement your BLEU evaluator from scratch.* There exist python libraries that implement BLEU for you. Do not use these.\n",
    "\n",
    "### Don't forget the written portion!\n",
    "\n",
    "A series of open-ended questions about these tasks are required for the written portion of this homework. Please see the homework instructions for this, as well as instructions about how to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_whVTquoc8CG"
   },
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2D919AKbc8CN",
    "outputId": "9b31f8f0-8bf6-4081-a500-984fcb0aaa9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NO MODIFY\n",
    "\n",
    "# this is useful for checking if your code is successfully using the GPU\n",
    "\n",
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mydevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kAmB5NAc8C5"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IooFVuqjc8DH"
   },
   "source": [
    "### Load dummy number reversal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "iP1hNN96dUOw",
    "outputId": "af75e00f-e93e-4b65-846a-d7521c8e7f6f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Load data from Google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5ANyhUkc8DO"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "train_path = 'data/toy_reverse/train/data.txt'\n",
    "dev_path = 'data/toy_reverse/dev/data.txt'\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaAYXBG-c8De"
   },
   "source": [
    "### 1. Load the e2e data\n",
    "\n",
    "Load in the E2E data similar to how the dummy number reversal dataset is loaded. That is, use the same `torchtext.data.Field` and `torchtext.data.TabularDataset` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2282IQ0Hc8Df"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE FOR LOADING THE E2E DATA HERE\n",
    "train_path = 'data/e2e-dataset/trainset.csv'\n",
    "dev_path = 'data/e2e-dataset/devset.csv'\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    tokenize=lambda row: row.split(', '),\n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    tokenize=lambda row: row.split(),\n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='csv',\n",
    "        skip_header=True,\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='csv',\n",
    "        skip_header=True,\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZsQu3zGc8Dj"
   },
   "source": [
    "Have a look at the vocab and some example data points.\n",
    "\n",
    "*If you have loaded in the E2E dataset correctly, the code in the cell below should work without any modification.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "4T2As_8qc8Dl",
    "outputId": "50926909-40d7-4158-b3a7-8b0a951cc828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['<unk>', '<pad>', 'familyFriendly[yes]', 'area[riverside]', 'eatType[coffee shop]', 'familyFriendly[no]', 'area[city centre]', 'eatType[pub]', 'food[Japanese]', 'food[Italian]', 'food[Fast food]', 'food[French]', 'priceRange[moderate]', 'priceRange[less than £20]', 'customer rating[average]', 'customer rating[low]', 'priceRange[high]', 'customer rating[5 out of 5]', 'priceRange[more than £30]', 'food[Indian]']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['<unk>', '<pad>', 'is', '<eos>', '<sos>', 'a', 'The', 'the', 'in', 'near', 'of', 'and', 'food', 'customer', 'located', 'It', 'restaurant', 'has', 'coffee', 'price']\n",
      "\n",
      "num training examples: 42037\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['name[The Eagle]', 'priceRange[£20-25]', 'customer rating[high]', 'area[riverside]']\n",
      "tgt:\n",
      " ['<sos>', 'The', 'Eagle', 'in', 'riverside,', 'prices', '£20-25', 'high', 'customer', 'rating.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "src.build_vocab(data_train, max_size=50000)\n",
    "tgt.build_vocab(data_train, max_size=50000)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2WV3kTHc8Do"
   },
   "source": [
    "### Model definition and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChWURHmhc8Dp"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, myinput, hidden):\n",
    "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly5qydMKc8Ds"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # zero the gradients of the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "    \n",
    "    # pass the inputs through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                \n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UywEYqKNc8Dw"
   },
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
    "    print(f'Running {n_iters} epochs...')\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "\n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # note batch size of 1, just for simplicity\n",
    "    # DO NOT INCREASE THE BATCH SIZE\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=1,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=mydevice, repeat=False)\n",
    "    \n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "            \n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "            \n",
    "            # this is because we're not actually using the batches.\n",
    "            # batch size is 1 and this just selects that first one\n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "            \n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
    "                start = time.time()\n",
    "        \n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zjn6DR-Fc8Dz"
   },
   "source": [
    "### 2. Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XN5yLil1c8D0"
   },
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
    "decoder1 = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faW9JDFyc8D6"
   },
   "source": [
    "Here are some guidelines for how much training to expect. Note that these *guidelines*; they are not exact.\n",
    "\n",
    "Only 1 epoch is needed for the number reversal dataset. This produces near-perfect results, and should take less than 5 minutes to run on a CPU.\n",
    "\n",
    "To memorize ~5 examples of the e2e dataset, ~100 epochs are needed (with a high teacher forcing ratio). This produces near-perfect results.\n",
    "\n",
    "To train on the full e2e dataset, only 1 epoch is needed to see decent outputs on the training data. More are required to increase fluency and see improvements on the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "yQW-oI8vc8D7",
    "outputId": "223d352a-0a23-4a6e-a129-e064fdf0dd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 epochs...\n",
      "step: 1000\t avg loss: 4.31\t time for 1000 steps: 0.38 min\n",
      "step: 2000\t avg loss: 3.60\t time for 1000 steps: 0.39 min\n",
      "step: 3000\t avg loss: 3.42\t time for 1000 steps: 0.38 min\n",
      "step: 4000\t avg loss: 3.24\t time for 1000 steps: 0.38 min\n",
      "step: 5000\t avg loss: 3.10\t time for 1000 steps: 0.40 min\n",
      "step: 6000\t avg loss: 3.03\t time for 1000 steps: 0.39 min\n",
      "step: 7000\t avg loss: 3.07\t time for 1000 steps: 0.41 min\n",
      "step: 8000\t avg loss: 2.94\t time for 1000 steps: 0.39 min\n",
      "step: 9000\t avg loss: 2.84\t time for 1000 steps: 0.39 min\n",
      "step: 10000\t avg loss: 2.82\t time for 1000 steps: 0.40 min\n",
      "step: 11000\t avg loss: 2.72\t time for 1000 steps: 0.40 min\n",
      "step: 12000\t avg loss: 2.66\t time for 1000 steps: 0.39 min\n",
      "step: 13000\t avg loss: 2.60\t time for 1000 steps: 0.40 min\n",
      "step: 14000\t avg loss: 2.65\t time for 1000 steps: 0.39 min\n",
      "step: 15000\t avg loss: 2.64\t time for 1000 steps: 0.39 min\n",
      "step: 16000\t avg loss: 2.62\t time for 1000 steps: 0.39 min\n",
      "step: 17000\t avg loss: 2.57\t time for 1000 steps: 0.40 min\n",
      "step: 18000\t avg loss: 2.57\t time for 1000 steps: 0.39 min\n",
      "step: 19000\t avg loss: 2.49\t time for 1000 steps: 0.39 min\n",
      "step: 20000\t avg loss: 2.48\t time for 1000 steps: 0.40 min\n",
      "step: 21000\t avg loss: 2.51\t time for 1000 steps: 0.39 min\n",
      "step: 22000\t avg loss: 2.51\t time for 1000 steps: 0.39 min\n",
      "step: 23000\t avg loss: 2.52\t time for 1000 steps: 0.39 min\n",
      "step: 24000\t avg loss: 2.48\t time for 1000 steps: 0.39 min\n",
      "step: 25000\t avg loss: 2.51\t time for 1000 steps: 0.40 min\n",
      "step: 26000\t avg loss: 2.45\t time for 1000 steps: 0.40 min\n",
      "step: 27000\t avg loss: 2.39\t time for 1000 steps: 0.39 min\n",
      "step: 28000\t avg loss: 2.43\t time for 1000 steps: 0.39 min\n",
      "step: 29000\t avg loss: 2.40\t time for 1000 steps: 0.40 min\n",
      "step: 30000\t avg loss: 2.40\t time for 1000 steps: 0.40 min\n",
      "step: 31000\t avg loss: 2.42\t time for 1000 steps: 0.39 min\n",
      "step: 32000\t avg loss: 2.42\t time for 1000 steps: 0.39 min\n",
      "step: 33000\t avg loss: 2.43\t time for 1000 steps: 0.39 min\n",
      "step: 34000\t avg loss: 2.43\t time for 1000 steps: 0.40 min\n",
      "step: 35000\t avg loss: 2.39\t time for 1000 steps: 0.40 min\n",
      "step: 36000\t avg loss: 2.31\t time for 1000 steps: 0.40 min\n",
      "step: 37000\t avg loss: 2.39\t time for 1000 steps: 0.39 min\n",
      "step: 38000\t avg loss: 2.41\t time for 1000 steps: 0.39 min\n",
      "step: 39000\t avg loss: 2.32\t time for 1000 steps: 0.39 min\n",
      "step: 40000\t avg loss: 2.31\t time for 1000 steps: 0.40 min\n",
      "step: 41000\t avg loss: 2.28\t time for 1000 steps: 0.40 min\n",
      "step: 42000\t avg loss: 2.36\t time for 1000 steps: 0.39 min\n",
      "End of epoch 0, avg loss 2.65\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "# but be sure that it prints some indication of how training is progressing\n",
    "\n",
    "trainIters(encoder1, decoder1, 1, print_every=1000, teacher_forcing_ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4yXnma6c8D-"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE FOR SAVING YOUR MODEL HERE\n",
    "ENCODER_PATH = 'encoder.mdl'\n",
    "DECODER_PATH = 'decoder.mdl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DaKIznPI4TSc"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), ENCODER_PATH)\n",
    "torch.save(decoder1.state_dict(), DECODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mT9NKZTQ4QMg",
    "outputId": "ddc0fb01-0295-415d-f2cf-019fd201470d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We encourage you to confirm that you can load your trained model here also\n",
    "encoder = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
    "decoder = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)\n",
    "encoder.load_state_dict(torch.load(ENCODER_PATH))\n",
    "decoder.load_state_dict(torch.load(DECODER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Cnz2XE0c8EB"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            next_word = output_vocab.itos[topi.item()]\n",
    "            decoded_words.append(next_word)\n",
    "            if next_word == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4_Op_dJc8EI"
   },
   "source": [
    "### 3. Implement beam search evaluator\n",
    "\n",
    "Be sure to return all the output sequences (i.e. if the beam size is k, you should return k sequences) and their associated probabilities. You will need the associated probabilities to select the best performing sequence when calculating BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyeArfhZc8EJ"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE FOR BEAM SEARCH HERE\n",
    "\n",
    "# The output of this cell should be an example input from the dev set, \n",
    "# and three outputs from a beam search evaluator.\n",
    "\n",
    "def evaluate_beam_search(encoder, decoder, sentence, max_length=MAX_LEN, beam_size=3):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Above are the same with greedy evaluate.\n",
    "####################################################################\n",
    "        # Initialize the beams\n",
    "        beams = [] #First element is sequence of tokens, second is corresponding log likelihood.\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(beam_size)\n",
    "        for i in range(beam_size):\n",
    "            beams.append([[topi[0][i]], topv[0][i].item(), decoder_hidden])\n",
    "\n",
    "        # Since above has run 1 decoder, here run max_length-1 times.\n",
    "        for di in range(max_length-1):\n",
    "            candidates = [] # Store k*k candidate seqs.\n",
    "            for ibeam in beams:\n",
    "                decoder_input = ibeam[0][-1] # Take last word in beam seq as input\n",
    "                # If current beam has ended, add it to candidates without expanding\n",
    "                if output_vocab.itos[decoder_input] == EOS_TOKEN:\n",
    "                    candidates.append(ibeam)\n",
    "                    continue \n",
    "                decoder_hidden = ibeam[2]\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.data.topk(beam_size)\n",
    "                # Add the new words to seqs\n",
    "                for i in range(beam_size):\n",
    "                    candidates.append([ibeam[0]+[topi[0][i]], ibeam[1]+topv[0][i].item(), decoder_hidden])\n",
    "                    # Since the output of decoder is log likelihood, use summation.\n",
    "            # Choose top k seqs according to length-normalized log-likelihood.\n",
    "            if candidates:\n",
    "                beams = sorted(candidates, key=lambda x:x[1]/len(x[0]), reverse=True)[:beam_size]\n",
    "\n",
    "        # Transform tokens into words:\n",
    "        decoded_words = []\n",
    "        for i in range(beam_size):\n",
    "            tmp_words = []\n",
    "            for j in beams[i][0]:\n",
    "                tmp_words.append(output_vocab.itos[j])\n",
    "            decoded_words.append(tmp_words)\n",
    "    \n",
    "        # Return the input from the dev set, and three outputs from a beam search evaluator\n",
    "        return sentence, decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7I4NKdEc8EM"
   },
   "source": [
    "Have a look at some generated sequences! This is the fun part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "D6toQm7Wc8EQ",
    "outputId": "7737da1a-1b78-48d3-8af6-3fe7b0cce06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name[The Punter]', 'food[Italian]', 'priceRange[high]']\n",
      "<sos> The Punter is a high priced Italian restaurant. <eos>\n",
      "\n",
      "<sos> The Punter serves Italian food. <eos>\n",
      "<sos> The Punter is an Italian restaurant. <eos>\n",
      "<sos> The Punter is an Italian restaurant. It is located near the river. <eos>\n",
      "['name[Giraffe]', 'eatType[coffee shop]', 'priceRange[high]', 'customer rating[average]', 'familyFriendly[yes]', 'near[The Bakers]']\n",
      "<sos> Giraffe is a coffee shop with a high price range and is near is and is average customer rating is and is friendly. <eos>\n",
      "\n",
      "<sos> Giraffe is a children friendly coffee shop located near The Bakers. It has a high price range and has an average customer rating. <eos>\n",
      "<sos> Giraffe is a children friendly coffee shop located near The Bakers. It has a high price range and has a high customer rating. <eos>\n",
      "<sos> Giraffe is a children friendly coffee shop located near The Bakers. It has a high price range and has an average rating. <eos>\n",
      "['name[Blue Spice]', 'eatType[coffee shop]', 'priceRange[less than £20]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[yes]', 'near[Avalon]']\n",
      "<sos> Blue Spice is a family friendly coffee shop located near the is in the city the is family friendly <eos>\n",
      "\n",
      "<sos> Blue Spice is an average family friendly coffee shop located near Café Brazil. <eos>\n",
      "<sos> Blue Spice is an average family friendly coffee shop located near Avalon in the city centre. <eos>\n",
      "<sos> Blue Spice is an average family friendly coffee shop located near Avalon in the city centre <eos>\n",
      "['name[Aromi]', 'eatType[pub]', 'customer rating[low]', 'area[city centre]', 'familyFriendly[no]']\n",
      "<sos> Aromi is a pub located in the city centre. a low customer rating is not family-friendly. <eos>\n",
      "\n",
      "<sos> In the city centre is a pub called Zizzi is located near the city centre. It is not family-friendly and has a low customer rating. <eos>\n",
      "<sos> In the city centre is a pub called Zizzi is located near the city centre. It is not family-friendly and has a low customer rating. It is located near Café Rouge. <eos>\n",
      "<sos> In the city centre is a pub called Zizzi is located near the city centre. It is not family-friendly and has a low customer rating. It is not family-friendly. <eos>\n",
      "['name[The Rice Boat]', 'food[Italian]', 'customer rating[average]', 'area[riverside]', 'familyFriendly[yes]']\n",
      "<sos> The Rice Boat is a family friendly Italian restaurant in the the the city in the <eos>\n",
      "\n",
      "<sos> The Rice Boat is an Italian restaurant in the riverside area with an average customer rating and is family friendly. <eos>\n",
      "<sos> The Rice Boat is an Italian restaurant in the riverside area with an average customer rating. <eos>\n",
      "<sos> The Rice Boat is an Italian restaurant in the riverside area with an average customer rating and is located in the <eos>\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "# This selects 5 random datapoints from the training data and shows the generated sequence\n",
    "\n",
    "for i in range(5):\n",
    "    item = random.choice(data_train.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    words_greedy = evaluate(encoder, decoder, seq)\n",
    "    print(' '.join(words_greedy))\n",
    "    print()\n",
    "    _, words = evaluate_beam_search(encoder, decoder, seq)\n",
    "    for iword in words:\n",
    "        print(' '.join(iword))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFKV338Lc8EU"
   },
   "source": [
    "### 4. Implement BLEU evaluator\n",
    "\n",
    "Remember that when calculating BLEU using beam search, select the top-scoring sequence output using the model probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE FOR THE BLEU EVALUATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIXCa-ZGfo_r"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# Preprocess the seq from data_dev, turn all letters to lower case and remove punctuations.\n",
    "def preprocessing(seq):\n",
    "    if EOS_TOKEN in seq:\n",
    "        seq.remove(EOS_TOKEN)\n",
    "    if SOS_TOKEN in seq:\n",
    "        seq.remove(SOS_TOKEN)\n",
    "    seq = [i.translate(str.maketrans('','',string.punctuation)).lower() for i in seq]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EzRuVq_Kc8EV",
    "outputId": "41c08f25-7b41-40d0-d97f-f4e7313235b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547\n"
     ]
    }
   ],
   "source": [
    "# Get the src-tgt dictionary  in dev data.\n",
    "dev_dict = {}\n",
    "for i in data_dev.examples:\n",
    "    key = tuple(i.src)\n",
    "    if key not in dev_dict:\n",
    "        dev_dict[key] = [preprocessing(i.tgt)]\n",
    "    else:\n",
    "        dev_dict[key] = dev_dict[key]+[preprocessing(i.tgt)]\n",
    "print(len(dev_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXHFnDRRRy7i"
   },
   "outputs": [],
   "source": [
    "# Calculate pn: modified precision scores for a set of ngrams.\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "def pn(src, tgt_list, n):\n",
    "    counts = Counter(ngrams(src,n))\n",
    "    max_counts = {}\n",
    "    for i in tgt_list:\n",
    "        reference_counts = Counter(ngrams(i,n))\n",
    "        for ngram in counts:\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "    clipped_counts = {ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()}\n",
    "    return sum(clipped_counts.values())/max(1, sum(counts.values())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahVGJWaWRy12"
   },
   "outputs": [],
   "source": [
    "# Calculate the Brevity Penalty\n",
    "import math\n",
    "def brevity_penalty(tgt_list, src_len):\n",
    "    ref_lens = [len(i) for i in tgt_list]\n",
    "    closest_ref_len = min(ref_lens, key = lambda ref_len: (abs(ref_len-src_len), ref_len))\n",
    "    if closest_ref_len <= src_len:\n",
    "        return 1\n",
    "    else:\n",
    "        return math.exp(1-closest_ref_len/src_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ynk529psd0C7"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU4\n",
    "def BLEU4(src, tgt_list):\n",
    "    p1 = pn(src, tgt_list, 1)\n",
    "    p2 = pn(src, tgt_list, 2)\n",
    "    p3 = pn(src, tgt_list, 3)\n",
    "    p4 = pn(src, tgt_list, 4)\n",
    "    if p1 == 0:\n",
    "        avg = 0\n",
    "    elif p2 == 0:\n",
    "        avg = p1\n",
    "    elif p3 == 0:\n",
    "        avg = (p1*p2)**0.5\n",
    "    elif p4 == 0:\n",
    "        avg = (p1*p1*p3)**(1/3)\n",
    "    else:\n",
    "        avg = (p1*p1*p3*p4)**(1/4)\n",
    "    return avg*brevity_penalty(tgt_list, len(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWVXuokf4kCa"
   },
   "outputs": [],
   "source": [
    "# The output of this cell should be the average BLEU score on the dev set\n",
    "# for greedy decoding AND for beam search decoding (beam size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eBGYKInYnOkk",
    "outputId": "50ca40b3-eeaf-4b0d-dc11-206e40ce1348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4393657025745546"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_greedy = []\n",
    "for key, value in dev_dict.items():\n",
    "    candidate = preprocessing(evaluate(encoder, decoder, key))\n",
    "    bleu_greedy.append(BLEU4(candidate, value))\n",
    "avg_bleu_greedy = sum(bleu_greedy)/len(bleu_greedy)\n",
    "print(avg_bleu_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eQJF6xfYn6i4",
    "outputId": "d0a8ddd0-c7eb-4674-af50-bd656078c509"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5431727027009272"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_beam = []\n",
    "for key, value in dev_dict.items():\n",
    "    _, candidates = evaluate_beam_search(encoder, decoder, key)\n",
    "    max_bleu = 0\n",
    "    for ican in candidates:\n",
    "        max_bleu = max(max_bleu, BLEU4(preprocessing(ican), value))\n",
    "    bleu_beam.append(max_bleu)\n",
    "avg_bleu_beam = sum(bleu_beam)/len(bleu_beam)\n",
    "print(avg_bleu_beam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx1PBvNNHvdU"
   },
   "source": [
    "## error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHQwPvpk73ew"
   },
   "source": [
    "### 2.6 unseen names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "QmcXEZYVKUzY",
    "outputId": "a9cebfc5-2b63-41ce-b2d1-55ecb9f9f70f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> The Mill is a coffee shop located near The near The coffee shop in the in the <eos>\n",
      "<sos> In the city centre, near The Sorrento, there is an Indian coffee shop named The Cambridge Blue. <eos>\n",
      "<sos> In the city centre, near The Sorrento, there is an Indian coffee shop named The Mill. <eos>\n",
      "<sos> In the city centre, near The Sorrento, there is a coffee shop called The Cambridge Blue. <eos>\n"
     ]
    }
   ],
   "source": [
    "test_name = ('name[Jings Malatang]','eatType[coffee shop]', 'priceRange[high]', 'area[city centre]', 'near[The Sorrento]')\n",
    "candidate = evaluate(encoder, decoder, test_name)\n",
    "print(\" \".join(candidate))\n",
    "_, candidates = evaluate_beam_search(encoder, decoder, test_name)\n",
    "for ican in candidates:\n",
    "    print(\" \".join(ican))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zCLCAm3rnDzo",
    "outputId": "0296ece3-f70c-4751-a391-aa5e5b58e767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 \n",
      "\n",
      "('name[Aromi]', 'eatType[coffee shop]', 'food[Chinese]', 'customer rating[low]', 'area[city centre]', 'familyFriendly[no]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Aromi is a coffee shop in the city centre. It city centre. It is not family-friendly. It is not family-friendly. <eos>\n",
      "0.30051459922164164\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the city centre, there is a family friendly coffee shop named Aromi. <eos>\n",
      "0.33778004875944534\n",
      "<sos> In the city centre, there is a family friendly coffee shop called Aromi. <eos>\n",
      "0.33778004875944534\n",
      "<sos> In the city centre, there is a family friendly coffee shop called Aromi. It is a low rated and rated one star rating. <eos>\n",
      "0.2323900400859313\n",
      "40 \n",
      "\n",
      "('name[Bibimbap House]', 'area[riverside]', 'near[Café Sicilia]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Near the riverside near the Café is called The Dumpling Tree is a family friendly family friendly coffee shop <eos>\n",
      "0.2760262237369417\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the riverside area near Clare Hall, is a family friendly restaurant called The Punter. <eos>\n",
      "0.3615855225145535\n",
      "<sos> In the riverside area near Clare Hall, is a family friendly restaurant named The Punter. <eos>\n",
      "0.34303014602642495\n",
      "<sos> In the riverside area near Clare Hall, is a family friendly restaurant called The Punter. It is a family friendly restaurant with a price range of 20 and <eos>\n",
      "0.2191767463521761\n",
      "60 \n",
      "\n",
      "('name[Browns Cambridge]', 'eatType[coffee shop]', 'food[English]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Crowne Plaza Hotel]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Browns Cambridge is a coffee shop located near the city Crowne the city It near the Crowne Plaza the in the city 5 the customer rating 5 of 5 out of 5 and is not family-friendly. 5 out of 5 by customers. <eos>\n",
      "0.20451245109882074\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the city centre near the Crowne Plaza Hotel, there is a coffee shop called Browns Cambridge. It serves English food and is rated 5 out of 5 by customers. <eos>\n",
      "0.46239484591627916\n",
      "<sos> In the city centre near the Crowne Plaza Hotel, there is a coffee shop called Browns Cambridge. It serves English food and is not family-friendly. 5 out of 5 by customers. <eos>\n",
      "0.43571576977260257\n",
      "<sos> In the city centre near the Crowne Plaza Hotel, there is a coffee shop called Browns Cambridge. It serves English food and has a 5 out of 5 customer rating. <eos>\n",
      "0.41994057601938223\n",
      "80 \n",
      "\n",
      "('name[Clowns]', 'eatType[coffee shop]', 'food[English]', 'customer rating[1 out of 5]', 'area[riverside]', 'near[Clare Hall]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Clowns is a coffee shop near Clare Hall in the the the riverside area the in the <eos>\n",
      "0.2850334739405197\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a coffee shop called Clowns is located near Clare Hall in the city centre. <eos>\n",
      "0.7327628488320422\n",
      "<sos> There is a coffee shop called Clowns is located near Clare Hall in the city centre. It serves English food. <eos>\n",
      "0.7459974853582002\n",
      "<sos> There is a coffee shop called Clowns is located near Clare Hall in the city centre. It serves English food and has a 5 out of 5 <eos>\n",
      "0.7199352992438696\n",
      "100 \n",
      "\n",
      "('name[Cocum]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'customer rating[low]', 'familyFriendly[yes]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Cocum is a family friendly coffee shop that provides low priced Chinese food. <eos>\n",
      "0.3448371180801331\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a family friendly coffee shop called The Vaults. It is low priced. <eos>\n",
      "0.3485275428624405\n",
      "<sos> There is a family friendly coffee shop called The Vaults. It is located near the river. <eos>\n",
      "0.3334477432809603\n",
      "<sos> There is a family friendly coffee shop called The Vaults. It is located near the city centre. <eos>\n",
      "0.3121234053102234\n",
      "120 \n",
      "\n",
      "('name[Cocum]', 'eatType[coffee shop]', 'food[English]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'familyFriendly[yes]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Cocum is a coffee shop that shop with a moderate price It is and is friendly. It is a 1 out of 5 <eos>\n",
      "0.42551621435268044\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a coffee shop called The Wrestlers. It is moderately priced and rated 1 out of 5 and is family friendly. <eos>\n",
      "0.48334056143388404\n",
      "<sos> There is a coffee shop called The Wrestlers. It is moderately priced and family friendly. <eos>\n",
      "0.5780890426382445\n",
      "<sos> There is a coffee shop called The Wrestlers. It is moderately priced and rated 1 out of 5 by customers. <eos>\n",
      "0.520153647144581\n",
      "140 \n",
      "\n",
      "('name[Cotto]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[more than £30]', 'customer rating[high]', 'area[riverside]', 'near[The Portland Arms]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Cotto is a coffee shop located near The Portland Arms in the Portland Arms a the price range It is It is of <eos>\n",
      "0.192252757218943\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a coffee shop named Cotto is located near The Portland Arms in the city centre. It serves Japanese food. It has a low customer rating and is price range of more than £30. <eos>\n",
      "0.30814978994183906\n",
      "<sos> There is a coffee shop named Cotto is located near The Portland Arms in the city centre. It serves Japanese food. It has a high customer rating and is price range of more than £30. <eos>\n",
      "0.447213595499958\n",
      "<sos> There is a coffee shop named Cotto is located near The Portland Arms in the city centre. <eos>\n",
      "0.2405867933490729\n",
      "160 \n",
      "\n",
      "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[£20-25]', 'customer rating[high]', 'area[riverside]', 'familyFriendly[no]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Fitzbillies is a coffee shop in the city It the in the the city It a It is It is range It is and is not not a is friendly. <eos>\n",
      "0.22667547112013808\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a coffee shop called Fitzbillies is located in the city centre. It is in the high price range and has a high customer rating. It is not child friendly. <eos>\n",
      "0.4230728420554421\n",
      "<sos> There is a coffee shop called Fitzbillies is located in the city centre. It is in the high price range and has a high customer rating. It is not children friendly. <eos>\n",
      "0.4230728420554421\n",
      "<sos> There is a coffee shop called Fitzbillies is located in the city centre. It is in the high price range and has a high customer rating. It is not kid friendly. <eos>\n",
      "0.4230728420554421\n",
      "180 \n",
      "\n",
      "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'customer rating[low]', 'area[riverside]', 'familyFriendly[no]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Fitzbillies is a low priced family friendly coffee shop located in the city centre. <eos>\n",
      "0.2661086980925717\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> Fitzbillies is a low priced family friendly coffee shop serving Chinese food. It is located in the city centre. <eos>\n",
      "0.5197930551447719\n",
      "<sos> Fitzbillies is a low priced family friendly coffee shop serving Chinese food. It is located in the city centre and has a low customer rating. <eos>\n",
      "0.5005012221099728\n",
      "<sos> Fitzbillies is a low priced family friendly coffee shop located in the city centre. <eos>\n",
      "0.3549753821666186\n",
      "200 \n",
      "\n",
      "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[English]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Fitzbillies is a cheap coffee shop in the city the the city 5 the the the city 5 centre. It is 5 and is not family-friendly. 5 out of 5 <eos>\n",
      "0.21201340644934405\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a cheap coffee shop named Fitzbillies is located in the city centre. It is not family-friendly. 5 out of 5 and is not family-friendly. <eos>\n",
      "0.3194920975782168\n",
      "<sos> There is a cheap coffee shop named Fitzbillies is located in the city centre. It is not family-friendly. 5 out of 5 and is not family friendly. <eos>\n",
      "0.31546589699521493\n",
      "<sos> There is a cheap coffee shop named Fitzbillies is located in the city centre. It is not family-friendly and has a 5 out of 5 customer rating. <eos>\n",
      "0.5076091691511417\n",
      "220 \n",
      "\n",
      "('name[Fitzbillies]', 'eatType[coffee shop]', 'food[English]', 'priceRange[moderate]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[no]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Fitzbillies is a moderately priced coffee shop in the city centre. It is the of the is and is not It is is of <eos>\n",
      "0.29793079067805034\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a moderately priced coffee shop serving English food in the riverside area named The Wrestlers. It is moderately priced and is rated 1 out of 5 by customers. <eos>\n",
      "0.4627458766446523\n",
      "<sos> There is a moderately priced coffee shop serving English food in the riverside area named The Wrestlers. It is moderately priced and is rated 1 out of 5 and is not child friendly. <eos>\n",
      "0.5200178122568863\n",
      "<sos> There is a moderately priced coffee shop serving English food in the riverside area named The Wrestlers. It is moderately priced and is rated 1 out of 5 and is not family-friendly. <eos>\n",
      "0.5251562603151827\n",
      "240 \n",
      "\n",
      "('name[Taste of Cambridge]', 'eatType[coffee shop]', 'food[English]', 'area[city centre]', 'familyFriendly[yes]', 'near[Crowne Plaza Hotel]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Taste of Cambridge is a coffee shop located near the city in the the city It near Crowne the city near Crowne the of Crowne the city It Crowne the of is and is is is <eos>\n",
      "0.1305226767827416\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a coffee shop called The Waterman, near the Crowne Plaza Hotel in the city centre. It serves English food and is not family-friendly. <eos>\n",
      "0.7280875500630786\n",
      "<sos> There is a coffee shop called The Waterman, near the Crowne Plaza Hotel in the city centre. It serves English food. It is not family-friendly. <eos>\n",
      "0.7120836715690464\n",
      "<sos> There is a coffee shop called The Waterman, near the Crowne Plaza Hotel in the city centre. It serves English food and is family-friendly. <eos>\n",
      "0.7734209459290874\n",
      "260 \n",
      "\n",
      "('name[The Cricketers]', 'eatType[coffee shop]', 'food[Chinese]', 'customer rating[5 out of 5]', 'familyFriendly[no]', 'near[The Portland Arms]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Cricketers is a coffee shop providing Japanese food Portland Arms and is rated 5 out of 5 and is not family-friendly. <eos>\n",
      "0.40069591431357804\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> Near The Portland Arms is a coffee shop called The Cricketers which serves Japanese food. It is not family friendly. <eos>\n",
      "0.45125541719761153\n",
      "<sos> Near The Portland Arms is a coffee shop called The Cricketers that serves Japanese food. It is not family friendly. <eos>\n",
      "0.45125541719761153\n",
      "<sos> Near The Portland Arms is a coffee shop called The Cricketers which serves Japanese food and is not family friendly. <eos>\n",
      "0.3791303841069397\n",
      "280 \n",
      "\n",
      "('name[The Eagle]', 'customer rating[5 out of 5]', 'area[riverside]', 'familyFriendly[no]', 'near[Café Brazil]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Eagle is a restaurant located in the city the It city 5 the near city 5 the near city 5 The customer rating out of 5 of 5 and is not <eos>\n",
      "0.20613214978037914\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> The Eagle is located near the city centre near Burger King. It has a 5 out of 5 customer rating and is not family-friendly. <eos>\n",
      "0.2714098990884678\n",
      "<sos> The Eagle is located near the city centre near Burger King. It has a 5 out of 5 rating and is not child friendly. <eos>\n",
      "0.23891238272317414\n",
      "<sos> The Eagle is located near the city centre near Burger King. It has a 5 out of 5 rating and is not family-friendly. <eos>\n",
      "0.2646017066426519\n",
      "300 \n",
      "\n",
      "('name[The Eagle]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[cheap]', 'customer rating[average]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Eagle is a cheap family friendly coffee shop located near Burger King. It provides Indian food. It is located in the city centre. <eos>\n",
      "0.24534246787477418\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> The Eagle is an Italian coffee shop located in the city centre near Burger King. It is family-friendly and has an average customer rating. <eos>\n",
      "0.5462957095179902\n",
      "<sos> The Eagle is an Italian coffee shop located in the city centre near Burger King. It is family-friendly and has a low customer rating. <eos>\n",
      "0.5597865239085812\n",
      "<sos> The Eagle is an Italian coffee shop located in the city centre near Burger King. It is family-friendly and has an average rating. <eos>\n",
      "0.5185265037896203\n",
      "320 \n",
      "\n",
      "('name[The Eagle]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[moderate]', 'customer rating[3 out of 5]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Eagle is a coffee shop providing Indian food in the moderate price range. It It is located in the city It city centre. It is and is kid friendly. It has a customer rating of 3 out of 5. <eos>\n",
      "0.4572430082372548\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> The Eagle is a moderately priced coffee shop in the city It city It near city It near Burger Burger King. King. It Burger King. King. It has a moderate price range and has a 3 out of 5 customer rating. It is kid friendly. <eos>\n",
      "0.4600651021172426\n",
      "<sos> The Eagle is a moderately priced coffee shop in the city It city It near city It near Burger Burger King. King. It Burger King. King. It has a moderate price range and has a 3 out of 5 customer rating. <eos>\n",
      "0.4541378043964257\n",
      "<sos> The Eagle is a moderately priced coffee shop in the city It city It near city It near Burger Burger King. King. It Burger King. King. It has a moderate price range and has a 3 out of 5 rating. <eos>\n",
      "0.4136399827570746\n",
      "340 \n",
      "\n",
      "('name[The Eagle]', 'eatType[coffee shop]', 'food[English]', 'priceRange[high]', 'customer rating[1 out of 5]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Eagle is a coffee shop in the city It It is It is is It is is is of is and is Burger is is Burger King. is <eos>\n",
      "0.1355816640980153\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the city centre near Burger King, there is a children friendly coffee shop named The Eagle. It serves English food. It has an average customer rating and is not child friendly. <eos>\n",
      "0.4036908285223282\n",
      "<sos> In the city centre near Burger King, there is a children friendly coffee shop named The Eagle. It serves English food. It has an average customer rating and is located in the city centre. <eos>\n",
      "0.43315360349245174\n",
      "<sos> In the city centre near Burger King, there is a children friendly coffee shop named The Eagle. It serves English food. It has an average customer rating and is located near The Portland Arms. <eos>\n",
      "0.39330425980621786\n",
      "360 \n",
      "\n",
      "('name[The Eagle]', 'eatType[coffee shop]', 'food[English]', 'priceRange[more than £30]', 'customer rating[high]', 'area[city centre]', 'familyFriendly[yes]', 'near[Burger King]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Eagle is a coffee shop in the city It city It It is is is is It is is is is and is is is Burger is Burger King. is is is is and is friendly. <eos>\n",
      "0.12906787032377806\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the city centre near Burger King, there is a children friendly coffee shop named The Eagle. It serves English food. It has a price range of more than £30 and has a customer rating of 5 out of 5. <eos>\n",
      "0.4523526915709248\n",
      "<sos> In the city centre near Burger King, there is a children friendly coffee shop named The Eagle. It serves English food. It has a price range of more than £30 and is rated 5 out of 5 by customers. <eos>\n",
      "0.4636183344875973\n",
      "<sos> In the city centre near Burger King, there is a children friendly coffee shop named The Eagle. It serves English food. It has a price range of more than £30 and has a customer rating of 5 out of 5 and is not child friendly. <eos>\n",
      "0.41419004760086103\n",
      "380 \n",
      "\n",
      "('name[The Golden Palace]', 'eatType[coffee shop]', 'food[English]', 'priceRange[£20-25]', 'customer rating[high]', 'area[city centre]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Golden Palace is a coffee shop in the city city centre. It a city centre. It is It is is and is <eos>\n",
      "0.24819194241699596\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the city centre, The Golden Palace is a highly rated coffee shop that serves English food. <eos>\n",
      "0.4884510833120414\n",
      "<sos> In the city centre, The Golden Palace is a highly rated coffee shop that serves English food. It is located in the city centre. <eos>\n",
      "0.5262659543458028\n",
      "<sos> In the city centre, The Golden Palace is a highly rated coffee shop that serves English food. It is located in the city centre and is rated highly by customers. <eos>\n",
      "0.40000728067131175\n",
      "400 \n",
      "\n",
      "('name[The Mill]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[high]', 'area[city centre]', 'near[The Sorrento]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Mill is a coffee shop located near The located in the The Sorrento. <eos>\n",
      "0.23801485533558359\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> The Mill is an Italian coffee shop located near The Sorrento in the city centre. <eos>\n",
      "0.5102602958840413\n",
      "<sos> The Mill is an Italian coffee shop located near The Sorrento. It is located in the city centre. <eos>\n",
      "0.53351431215728\n",
      "<sos> The Mill is an Italian coffee shop located near The Sorrento. It is located in the riverside. <eos>\n",
      "0.4085003112582802\n",
      "420 \n",
      "\n",
      "('name[The Phoenix]', 'customer rating[average]', 'area[city centre]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Phoenix is an average rated restaurant in the city centre city centre. <eos>\n",
      "0.22986864711558164\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> In the city centre, The Phoenix has an average customer rating and is located near the city centre. <eos>\n",
      "0.7315859649494699\n",
      "<sos> In the city centre, The Phoenix has an average customer rating and is not family-friendly. <eos>\n",
      "0.7421300793854203\n",
      "<sos> In the city centre, The Phoenix has an average customer rating. <eos>\n",
      "0.8349950232057651\n",
      "440 \n",
      "\n",
      "('name[The Punter]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[£20-25]', 'customer rating[high]', 'familyFriendly[no]', 'near[Café Sicilia]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Punter is a coffee shop providing Chinese food It is near Café is and is It is is is It is and is not friendly. It is and is not friendly. <eos>\n",
      "0.1512413119573581\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a coffee shop called The Punter near Café Sicilia that serves French food. It is not children friendly and has a price range of more than £30 and has a customer rating of 5 out of 5. <eos>\n",
      "0.17276705959145341\n",
      "<sos> There is a coffee shop called The Punter near Café Sicilia that serves French food. It is not children friendly and has a price range of more than £30. <eos>\n",
      "0.22242504381511566\n",
      "<sos> There is a coffee shop called The Punter near Café Sicilia that serves French food. It is not children friendly and has a price range of more than £30 and has a customer rating of 5 out of 5 <eos>\n",
      "0.17276705959145341\n",
      "460 \n",
      "\n",
      "('name[The Punter]', 'eatType[coffee shop]', 'food[English]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'familyFriendly[no]', 'near[Café Sicilia]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Punter is a cheap coffee shop near Café Sicilia. It is is 5 out of 5 and is not family-friendly. <eos>\n",
      "0.3391646586813851\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a cheap coffee shop called The Punter located near Café Sicilia. It serves English food and is not family-friendly. 5 out of 5 and is not family-friendly. <eos>\n",
      "0.5681509603211117\n",
      "<sos> There is a cheap coffee shop called The Punter located near Café Sicilia. It serves English food and is rated 5 out of 5 by customers. <eos>\n",
      "0.5652735468032671\n",
      "<sos> There is a cheap coffee shop called The Punter located near Café Sicilia. It serves English food and is rated 5 out of 5. <eos>\n",
      "0.5641321026977398\n",
      "480 \n",
      "\n",
      "('name[The Wrestlers]', 'customer rating[5 out of 5]', 'familyFriendly[yes]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Wrestlers is a family friendly five star family friendly restaurant. <eos>\n",
      "0.22773126241644526\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> There is a family friendly restaurant called The Wrestlers. It has a 5 out of 5 customer rating and is located at the city centre. <eos>\n",
      "0.42282306959394567\n",
      "<sos> There is a family friendly restaurant called The Wrestlers. It has a 5 out of 5 customer rating and is family friendly. <eos>\n",
      "0.4872664475880673\n",
      "<sos> There is a family friendly restaurant called The Wrestlers. It has a 5 out of 5 customer rating and is located near the river. <eos>\n",
      "0.44146898047006294\n",
      "500 \n",
      "\n",
      "('name[The Wrestlers]', 'eatType[coffee shop]', 'food[Chinese]', 'priceRange[less than £20]', 'area[riverside]', 'familyFriendly[no]', 'near[Raja Indian Cuisine]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Wrestlers is a family friendly coffee shop located near Raja Indian Cuisine. It is in the city centre. <eos>\n",
      "0.1908345380707581\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> Near Raja Indian Cuisine is The Wrestlers. It is a family friendly coffee shop serving Indian food. It is located in the city centre. <eos>\n",
      "0.5099786694609391\n",
      "<sos> Near Raja Indian Cuisine is The Wrestlers. It is a family friendly coffee shop serving Indian food. It is located in the city centre and is not family friendly. <eos>\n",
      "0.4822964300736037\n",
      "<sos> Near Raja Indian Cuisine is The Wrestlers. It is a family friendly coffee shop that serves French food. It is located in the city centre. <eos>\n",
      "0.48087628848609315\n",
      "520 \n",
      "\n",
      "('name[The Wrestlers]', 'eatType[coffee shop]', 'food[English]', 'priceRange[moderate]', 'area[riverside]', 'familyFriendly[yes]', 'near[Raja Indian Cuisine]')\n",
      "GREEDY PREDICT:\n",
      "<sos> The Wrestlers is a family friendly coffee shop that serves Japanese in the in the is It is of Raja <eos>\n",
      "0.2928106935986605\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> Near Raja Indian Cuisine is a family friendly coffee shop named The Wrestlers that serves Japanese food. It is located in the riverside area and is rated 1 out of 5 by customers. <eos>\n",
      "0.46689043688064635\n",
      "<sos> Near Raja Indian Cuisine is a family friendly coffee shop named The Wrestlers that serves Japanese food. It is located in the city centre. <eos>\n",
      "0.5099786694609391\n",
      "<sos> Near Raja Indian Cuisine is a family friendly coffee shop named The Wrestlers that serves Japanese food. It is located in the riverside area and is rated 5 out of 5. <eos>\n",
      "0.48826820844158064\n",
      "540 \n",
      "\n",
      "('name[Wildwood]', 'eatType[coffee shop]', 'food[English]', 'priceRange[£20-25]', 'customer rating[high]', 'near[Ranch]')\n",
      "GREEDY PREDICT:\n",
      "<sos> Wildwood is a coffee shop with a customer rating of 5 out of 5 near Ranch and is a is a the price <eos>\n",
      "0.19086696487666654\n",
      "\n",
      "\n",
      "BEAM PREDICT:\n",
      "<sos> Wildwood is a coffee shop with a high customer rating. It serves English food. It is located near Ranch. <eos>\n",
      "0.5373901198968006\n",
      "<sos> Wildwood is a coffee shop with a high customer rating. It serves English food. It is near Ranch. <eos>\n",
      "0.3593041119630842\n",
      "<sos> Wildwood is a coffee shop with a high customer rating. It serves English food. It is near Ranch and is a price range of £20-25. <eos>\n",
      "0.27916130958459484\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in dev_dict.items():\n",
    "    i+=1\n",
    "    if i%20!=0:continue\n",
    "    print(i,'\\n')\n",
    "    print(key)\n",
    "    candidate = evaluate(encoder, decoder, key)\n",
    "    print(\"GREEDY PREDICT:\")\n",
    "    print(\" \".join(candidate))\n",
    "    print(BLEU4(candidate, value))\n",
    "    print(\"\\n\")\n",
    "    print(\"BEAM PREDICT:\")\n",
    "    _, candidates = evaluate_beam_search(encoder, decoder, key)\n",
    "    for ican in candidates:\n",
    "        print(\" \".join(ican))\n",
    "        print( BLEU4(preprocessing(ican), value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqXeKwAlLZav"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
